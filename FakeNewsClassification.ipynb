{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FakeNewsClassification",
      "provenance": [],
      "mount_file_id": "1kYC4s_9pIQejz-s3t5K1hAYJB5sAFE_R",
      "authorship_tag": "ABX9TyOhd0cYFxCJAgLmu70u8q7B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ameyas1/FakeNewClassification/blob/master/FakeNewsClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieUxsTI5poID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "791e390b-f6ce-4c76-ff2f-de783e6bdf4e"
      },
      "source": [
        "!pip install trax==1.3.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting trax==1.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/d8/ad90a5c79804561bbbc5fd65a4cb6b6e735370225e777cfc46980a9dc479/trax-1.3.1-py2.py3-none-any.whl (347kB)\n",
            "\r\u001b[K     |█                               | 10kB 24.7MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30kB 3.8MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |████▊                           | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 71kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 81kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 92kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 102kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 112kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 122kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 133kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 143kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 153kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 163kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 174kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 184kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 194kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 204kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 215kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 225kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 235kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 245kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 256kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 266kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 276kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 286kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 296kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 307kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 317kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 327kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 337kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 348kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (0.1.75)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (0.17.2)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (2.1.0)\n",
            "Collecting t5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/77/c00ce95121f5b8363b880aec38fde71ecaf9b7eeb29ed8bd29fc5f5b8541/t5-0.6.4-py3-none-any.whl (163kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 14.4MB/s \n",
            "\u001b[?25hCollecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (0.8.1)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (0.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (1.15.0)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (0.1.52)\n",
            "Collecting tensorflow-text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b2/2dbd90b93913afd07e6101b8b84327c401c394e60141c1e98590038060b3/tensorflow_text-2.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 14.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from trax==1.3.1) (1.4.1)\n",
            "Collecting tensor2tensor\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/7c/9e87d30cefad5cbc390bb7f626efb3ded9b19416b8160f1a1278da81b218/tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 40.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax->trax==1.3.1) (3.3.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax==1.3.1) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax==1.3.1) (1.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (0.3.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (2.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (1.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (0.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (0.16.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (3.12.4)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax==1.3.1) (20.1.0)\n",
            "Collecting mesh-tensorflow[transformer]>=0.1.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/6b/d01247d9a9b49389599a28e17782ec1d3d45413cedbfe8e90aad46835329/mesh_tensorflow-0.1.16-py3-none-any.whl (305kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 48.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from t5->trax==1.3.1) (1.0.5)\n",
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Collecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/01/5291749a13f16eaaeb33d4c5b52adbcb81ae0dda393ceba2cb206b151a88/tfds_nightly-3.2.1.dev202009070105-py3-none-any.whl (3.5MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5MB 47.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from t5->trax==1.3.1) (3.2.5)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/d3/be980ad7cda7c4bbfa97ee3de062fb3014fc1a34d6dd5b82d7b92f8d6522/sacrebleu-1.4.13-py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from t5->trax==1.3.1) (0.22.2.post1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from t5->trax==1.3.1) (1.6.0+cu101)\n",
            "Collecting transformers>=2.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 51.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 48.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: babel in /usr/local/lib/python3.6/dist-packages (from t5->trax==1.3.1) (2.8.0)\n",
            "Requirement already satisfied: tensorflow<2.4,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->trax==1.3.1) (2.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (2.10.0)\n",
            "Collecting pypng\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/fb/f719f1ac965e2101aa6ea6f54ef8b40f8fbb033f6ad07c017663467f5147/pypng-0.0.20.tar.gz (649kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 50.6MB/s \n",
            "\u001b[?25hCollecting bz2file\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Collecting gevent\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/18/3932900a42d7010cc63529fc5cb7a5a20fb61878d1721d0d4387567d5973/gevent-20.6.2-cp36-cp36m-manylinux2010_x86_64.whl (5.3MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3MB 52.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (1.1.1)\n",
            "Collecting gunicorn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/ca/926f7cd3a2014b16870086b2d0fdc84a9e49473c68a8dff8b57f7c156f43/gunicorn-20.0.4-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.6MB/s \n",
            "\u001b[?25hCollecting tensorflow-probability==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/3a/c10b6c22320531c774402ac7186d1b673374e2a9d12502cbc8d811e4601c/tensorflow_probability-0.7.0-py2.py3-none-any.whl (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 53.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (1.7.12)\n",
            "Collecting tf-slim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 45.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (1.1.2)\n",
            "Collecting tensorflow-gan\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/2e/62922111d7d50e1900e3030764743ea7735540ce103b3ab30fd5cd2d8a2b/tensorflow_gan-2.0.0-py2.py3-none-any.whl (365kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 48.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (4.1.2.30)\n",
            "Collecting kfac\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/34/d2ff705c34a6250498e41766fa682fb0be168b2f2940cd20e5e2a4f40261/kfac-0.2.2-py2.py3-none-any.whl (191kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 52.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (7.0.0)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (0.8.3)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (4.1.3)\n",
            "Requirement already satisfied: dopamine-rl in /usr/local/lib/python3.6/dist-packages (from tensor2tensor->trax==1.3.1) (1.0.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax==1.3.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax==1.3.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax==1.3.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax==1.3.1) (2020.6.20)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax==1.3.1) (1.52.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->trax==1.3.1) (49.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->t5->trax==1.3.1) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->t5->trax==1.3.1) (2018.9)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5->trax==1.3.1) (0.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5->trax==1.3.1) (0.1.5)\n",
            "Collecting importlib-resources; python_version < \"3.9\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/03/0f9595c0c2ef12590877f3c47e5f579759ce5caf817f8256d5dcbd8a1177/importlib_resources-3.0.0-py2.py3-none-any.whl\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->t5->trax==1.3.1) (0.16.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax==1.3.1) (20.4)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 31.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax==1.3.1) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax==1.3.1) (3.0.12)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (0.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (2.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (1.31.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (0.35.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (0.3.3)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/33/565274c28a11af60b7cfc0519d46bde4125fcd7d32ebc0a81b480d0e8da6/zope.interface-5.1.0-cp36-cp36m-manylinux2010_x86_64.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 51.4MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/96/361edb421a077a4c208b4a5c212737d78ae03ce67fbbcd01621c49f332d1/zope.event-4.4-py2.py3-none-any.whl\n",
            "Collecting greenlet>=0.4.16; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/a4/0d8685c98986326534b0753a8b92b3082bc9df42b348bc50d6c69839c9f9/greenlet-0.4.16-cp36-cp36m-manylinux1_x86_64.whl (44kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.6/dist-packages (from sympy->tensor2tensor->trax==1.3.1) (1.1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.7.0->tensor2tensor->trax==1.3.1) (4.4.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor->trax==1.3.1) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor->trax==1.3.1) (0.0.4)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor->trax==1.3.1) (0.17.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client->tensor2tensor->trax==1.3.1) (1.17.2)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax==1.3.1) (2.11.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax==1.3.1) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax==1.3.1) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask->tensor2tensor->trax==1.3.1) (1.0.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gan->tensor2tensor->trax==1.3.1) (0.9.0)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tensor2tensor->trax==1.3.1) (2.7.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor->trax==1.3.1) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor->trax==1.3.1) (4.6)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tensor2tensor->trax==1.3.1) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tfds-nightly->t5->trax==1.3.1) (3.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.7.0->t5->trax==1.3.1) (2.4.7)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (3.2.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client->tensor2tensor->trax==1.3.1) (4.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask->tensor2tensor->trax==1.3.1) (1.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (1.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax==1.3.1) (3.1.0)\n",
            "Building wheels for collected packages: pypng, bz2file, sacremoses\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypng: filename=pypng-0.0.20-cp36-none-any.whl size=67162 sha256=53da27dd1f25e1a684a060a1e69454b9cc4f2cd686835feec1b0d60323d4f447\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/6b/ef/0493b536b6d4722c2ae9486691b1d49b922b9877922beeabb3\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bz2file: filename=bz2file-0.98-cp36-none-any.whl size=6884 sha256=b5ae62880db35a8f0babf1fea8e6aece0c3542f10248c707b95425af1840587a\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=1eda3af88b121f45beb9d5bbc5b5ee2221cb77dedfd63e8ce5909c3634399c54\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built pypng bz2file sacremoses\n",
            "\u001b[31mERROR: kfac 0.2.2 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: mesh-tensorflow, rouge-score, importlib-resources, tfds-nightly, portalocker, sacrebleu, sentencepiece, tokenizers, sacremoses, transformers, tensorflow-text, t5, funcsigs, pypng, bz2file, zope.interface, zope.event, greenlet, gevent, gunicorn, tensorflow-probability, tf-slim, tensorflow-gan, kfac, tensor2tensor, trax\n",
            "  Found existing installation: tensorflow-probability 0.11.0\n",
            "    Uninstalling tensorflow-probability-0.11.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.11.0\n",
            "Successfully installed bz2file-0.98 funcsigs-1.0.2 gevent-20.6.2 greenlet-0.4.16 gunicorn-20.0.4 importlib-resources-3.0.0 kfac-0.2.2 mesh-tensorflow-0.1.16 portalocker-2.0.0 pypng-0.0.20 rouge-score-0.0.4 sacrebleu-1.4.13 sacremoses-0.0.43 sentencepiece-0.1.91 t5-0.6.4 tensor2tensor-1.15.7 tensorflow-gan-2.0.0 tensorflow-probability-0.7.0 tensorflow-text-2.3.0 tf-slim-1.1.0 tfds-nightly-3.2.1.dev202009070105 tokenizers-0.8.1rc2 transformers-3.1.0 trax-1.3.1 zope.event-4.4 zope.interface-5.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "albYWjB-qDBX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13b5fc93-8fa4-48a3-971a-3c1cb16db8b0"
      },
      "source": [
        "import pandas as pd\n",
        "import random as rnd\n",
        "import trax\n",
        "trax.supervised.trainer_lib.init_random_number_generators(31)\n",
        "import trax.fastmath.numpy as fastnp\n",
        "from trax import layers as tl"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMhmXaScqXxA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 984
        },
        "outputId": "fdd123d9-4a1a-4427-8801-db54b59fc1bf"
      },
      "source": [
        "!pip install plotly\n",
        "!pip install --upgrade nbformat\n",
        "!pip install nltk\n",
        "!pip install spacy # spaCy is an open-source software library for advanced natural language processing\n",
        "!pip install WordCloud\n",
        "!pip install gensim # Gensim is an open-source library for unsupervised topic modeling and natural language processing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# import tensorflow as tf\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "# import keras\n",
        "# from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional\n",
        "# from tensorflow.keras.models import Model\n",
        "# from jupyterthemes import jtplot\n",
        "# jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False) \n",
        "# setting the style of the notebook to be monokai theme  \n",
        "# this line of code is important to ensure that we are able to see the x and y axes clearly\n",
        "# If you don't run this code line, you will notice that the xlabel and ylabel on any plot is black on black and it will be hard to see them. \n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (4.4.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly) (1.15.0)\n",
            "Requirement already up-to-date: nbformat in /usr/local/lib/python3.6/dist-packages (5.0.7)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat) (4.3.3)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat) (4.6.3)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1->nbformat) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1->nbformat) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (49.6.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.7.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Requirement already satisfied: WordCloud in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from WordCloud) (1.18.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from WordCloud) (7.0.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.1.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.14.48)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.17.48)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sH1cLayvlR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU88urgnsXhj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a9234b9a-1371-45a2-8a50-9d5e8b7797ef"
      },
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idym9WhlszfJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "0fb64b90-3288-4648-de09-2acf5a2cc63e"
      },
      "source": [
        "!kaggle competitions download -c fake-news"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test.csv.zip to /content\n",
            " 53% 5.00M/9.42M [00:00<00:00, 26.4MB/s]\n",
            "100% 9.42M/9.42M [00:00<00:00, 37.3MB/s]\n",
            "Downloading submit.csv to /content\n",
            "  0% 0.00/40.6k [00:00<?, ?B/s]\n",
            "100% 40.6k/40.6k [00:00<00:00, 38.5MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 89% 33.0M/37.0M [00:00<00:00, 12.8MB/s]\n",
            "100% 37.0M/37.0M [00:00<00:00, 47.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO_UDIpSs73h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "a42dcbdf-9f0e-454d-eba9-c11a003cec32"
      },
      "source": [
        "!unzip /content/train.csv.zip\n",
        "!unzip /content/test.csv.zip"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  /content/test.csv.zip\n",
            "  inflating: test.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuWGwesxqmPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JxQEbcutQsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df[[\"text\",\"label\"]]\n",
        "# test_df=test_df[[\"text\"]]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjGedmAOtLIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8c6fd9c4-7f4c-436f-b6ca-949b0c7fa055"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ever get the feeling your life circles the rou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1\n",
              "1  Ever get the feeling your life circles the rou...      0\n",
              "2  Why the Truth Might Get You Fired October 29, ...      1\n",
              "3  Videos 15 Civilians Killed In Single US Airstr...      1\n",
              "4  Print \\nAn Iranian woman has been sentenced to...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC8kXrblyepn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d09cd4bf-2074-46c0-87fd-abb20e364dd8"
      },
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text     39\n",
              "label     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtb4o2Rvymlg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.dropna(inplace=True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPXQsZ-KtOUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_pos = df[df.label==1]\n",
        "all_neg = df[df.label==0]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5FcgzuLtwpm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c225b1f9-a5df-4860-a304-6746d50f719a"
      },
      "source": [
        "len(all_pos),len(all_neg)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10374, 10387)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS-idazDueYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_pos_text = all_pos.text.to_list()\n",
        "all_neg_text = all_neg.text.to_list()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkePUOD8vPZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_pos, val_pos = train_test_split(all_pos_text, test_size=0.13)\n",
        "train_neg, val_neg = train_test_split(all_neg_text, test_size=0.13)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4Ig48_WwPKQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "858e046d-3719-40bd-98d9-c6df97d33369"
      },
      "source": [
        "train_x = train_pos + train_neg \n",
        "val_x  = val_pos + val_neg\n",
        "\n",
        "train_y = fastnp.append(fastnp.ones(len(train_pos)), fastnp.zeros(len(train_neg)))\n",
        "val_y  = fastnp.append(fastnp.ones(len(val_pos)), fastnp.zeros(len(val_neg)))\n",
        "print(f\"length of train_x {len(train_x)}\")\n",
        "print(f\"length of val_x {len(val_x)}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of train_x 18061\n",
            "length of val_x 2700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_QuQvsBw0V_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "494e2276-a099-48ef-9de3-755da0a46673"
      },
      "source": [
        "# download stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZmiFXBFxPTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove stopwords and remove words with 2 or less characters\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    text = re.sub(r'#', '', text)\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words and token not in string.punctuation:\n",
        "            result.append(token)\n",
        "            \n",
        "    return result"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCW1MMI9xYzj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        },
        "outputId": "13a87a0b-2293-437c-97ca-bef52c512f5e"
      },
      "source": [
        "# Try out function that processes tweets\n",
        "print(\"original tweet at training position 0\")\n",
        "print(train_pos[0])\n",
        "\n",
        "print(\"Tweet at training position 0 after processing:\")\n",
        "preprocess(train_pos[0])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original tweet at training position 0\n",
            "VICE News Fri, November 18, 2016 10:38am URL: Embed: \n",
            "With two avowed supporters of the anti-abortion movement set to take the White House in just 63 days, Planned Parenthood will have to prepare for deep cuts and open hostility. \n",
            "Caroline Modarressy-Tehrani went to Washington D.C. where Planned Parenthood’s leadership met behind closed doors to mount its counter-attack. \n",
            "Watch VICE News Tonight weeknights at 7:30 on HBO. \n",
            "Subscribe to VICE News here: http://bit.ly/Subscribe-to-VICE-News \n",
            "Check out VICE News for more: http://vicenews.com \n",
            "Follow VICE News here: \n",
            "Tweet at training position 0 after processing:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['vice',\n",
              " 'news',\n",
              " 'november',\n",
              " 'embed',\n",
              " 'avowed',\n",
              " 'supporters',\n",
              " 'anti',\n",
              " 'abortion',\n",
              " 'movement',\n",
              " 'white',\n",
              " 'house',\n",
              " 'days',\n",
              " 'planned',\n",
              " 'parenthood',\n",
              " 'prepare',\n",
              " 'deep',\n",
              " 'cuts',\n",
              " 'open',\n",
              " 'hostility',\n",
              " 'caroline',\n",
              " 'modarressy',\n",
              " 'tehrani',\n",
              " 'went',\n",
              " 'washington',\n",
              " 'planned',\n",
              " 'parenthood',\n",
              " 'leadership',\n",
              " 'closed',\n",
              " 'doors',\n",
              " 'mount',\n",
              " 'counter',\n",
              " 'attack',\n",
              " 'watch',\n",
              " 'vice',\n",
              " 'news',\n",
              " 'tonight',\n",
              " 'weeknights',\n",
              " 'subscribe',\n",
              " 'vice',\n",
              " 'news',\n",
              " 'check',\n",
              " 'vice',\n",
              " 'news',\n",
              " 'follow',\n",
              " 'vice',\n",
              " 'news']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O6KZEpyxdZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a9e1d09f-da98-4424-9c59-b573a76a109e"
      },
      "source": [
        "# Build the vocabulary\n",
        "# Unit Test Note - There is no test set here only train/val\n",
        "\n",
        "# Include special tokens \n",
        "# started with pad, end of line and unk tokens\n",
        "Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
        "# Note that we build vocab using training data\n",
        "for x in train_x: \n",
        "    processed_tweet = preprocess(x)\n",
        "    for word in processed_tweet:\n",
        "        if word not in Vocab: \n",
        "            Vocab[word] = len(Vocab)\n",
        "    \n",
        "print(\"Total words in vocab are\",len(Vocab))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words in vocab are 154878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N35MVpGbyChF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_to_tensor(text, vocab_dict, unk_token='__UNK__', verbose=False):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet - A string containing a tweet\n",
        "        vocab_dict - The words dictionary\n",
        "        unk_token - The special string for unknown tokens\n",
        "        verbose - Print info durign runtime\n",
        "    Output:\n",
        "        tensor_l - A python list with\n",
        "        \n",
        "    '''   \n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    # Process the tweet into a list of words\n",
        "    # where only important words are kept (stop words removed)\n",
        "    word_l = preprocess(text)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"List of words from the processed tweet:\")\n",
        "        print(word_l)\n",
        "        \n",
        "    # Initialize the list that will contain the unique integer IDs of each word\n",
        "    tensor_l = []\n",
        "    \n",
        "    # Get the unique integer ID of the __UNK__ token\n",
        "    unk_ID = vocab_dict[unk_token]\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
        "        \n",
        "    # for each word in the list:\n",
        "    for word in word_l:\n",
        "        \n",
        "        # Get the unique integer ID.\n",
        "        # If the word doesn't exist in the vocab dictionary,\n",
        "        # use the unique ID for __UNK__ instead.\n",
        "        word_ID = vocab_dict.get(word,unk_ID)\n",
        "    ### END CODE HERE ###\n",
        "        \n",
        "        # Append the unique integer ID to the tensor list.\n",
        "        tensor_l.append(word_ID) \n",
        "    \n",
        "    return tensor_l"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfz009tBzc19",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "46722712-41de-4a4f-e8ff-ba86702c9290"
      },
      "source": [
        "print(\"Actual tweet is\\n\", val_pos[0])\n",
        "print(\"\\nTensor of tweet:\\n\", text_to_tensor(val_pos[0], vocab_dict=Vocab))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual tweet is\n",
            " Trump Mistakes Ex-Marine Black Supporter For Protester, Calls Him A ‘Thug’ (TWEET/VIDEO) ‹ › Trump Says Alec Baldwin’s Impression Of Him Is Unfair: ‘He’s Portraying Someone Mean And Nasty’ (VIDEO) By Andrew Bradford on October 29, 2016 Subscribe \n",
            "Donald Trump doesn’t have a sense of humor. While he may smile from time to time, it always seems forced, as if he’s afraid his face will crack if he smiles too broadly. And now he’s whining yet again about Alec Baldwin’s brilliant portrayal of him on Saturday Night Live. \n",
            "As part of an interview with AJ Calloway of Extra , Trump had this to say about the way Baldwin plays him: “Well, I think I’m a much nicer guy than he’s portraying. He’s portraying someone who’s very mean and nasty, and I’m not mean and nasty. I think I’m a much nicer person than he’s portraying, so I think it’s an inaccurate portrayal of me.” \n",
            "Clearly, in addition to Trump not having a sense of humor, he also is deficient in the self-awareness department. \n",
            "Would the Donald ever do SNL again? Not a chance, he declared: “They want me to go back. No, I’m not interested in going back…They’re making me out to be a very mean, bad kind of a guy and that’s not me.” \n",
            "Trump was also asked about the fact that NBC had fired Billy Bush due to the leaked videotape in which Trump said he likes to grab women “by the pussy” and kiss them against their will. The GOP nominee cryptically responded that “they did it to him,” but would not elaborate on who “they” might be. He then added : “I have always liked Billy, but I have not [spoken to him]…but I always found Billy to be a good guy.” \n",
            "Yeah, a good guy who laughed when you brazenly talked about how you get off on sexually assaulting women because you happen to be wealthy and famous. \n",
            "Face it, Donald, you’re a pig. And to make matters worse, you can’t even laugh at yourself. \n",
            "Featured Image Via Screengrab About Andrew Bradford \n",
            "Andrew Bradford is a single father who lives in Atlanta. A member of the Christian Left, he has worked in the fields of academia, journalism, and political consulting. His passions are art, music, food, and literature. He believes in equal rights and justice for all. To see what else he likes to write about, check out his blog at Deepleftfield.info. Connect\n",
            "\n",
            "Tensor of tweet:\n",
            " [428, 1598, 2889, 44, 4014, 18968, 2188, 12556, 1801, 1363, 428, 1478, 31948, 16321, 3281, 8003, 22928, 1987, 9428, 1363, 10280, 13325, 55, 36, 427, 428, 560, 18089, 23411, 142, 142, 5195, 7508, 5092, 9166, 26788, 16630, 9107, 31948, 16321, 1847, 2599, 7152, 351, 885, 1377, 148881, 8005, 428, 16321, 4093, 104, 8464, 22928, 22928, 1987, 9428, 1987, 9428, 104, 8464, 471, 22928, 104, 12304, 2599, 2610, 1030, 428, 560, 18089, 20842, 106, 1135, 348, 427, 1699, 1365, 497, 1782, 772, 847, 1987, 99, 428, 368, 586, 8761, 6624, 1354, 7514, 35038, 428, 233, 3262, 1668, 795, 5806, 26900, 1962, 41766, 2090, 7308, 537, 16461, 6624, 2495, 6624, 622, 1768, 622, 21708, 22927, 1875, 15965, 27521, 795, 563, 7989, 3804, 5092, 427, 775, 276, 6548, 77, 3773, 13513, 10280, 13325, 10280, 13325, 1339, 978, 893, 17932, 284, 3956, 2292, 2286, 1550, 7708, 11492, 331, 2283, 18856, 11045, 2551, 9914, 2352, 1318, 2804, 1741, 3262, 122, 37, 1798, 49593, 494, 128]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5v87f3Nzg2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
        "    '''\n",
        "    Input: \n",
        "        data_pos - Set of posstive examples\n",
        "        data_neg - Set of negative examples\n",
        "        batch_size - number of samples per batch. Must be even\n",
        "        loop - True or False\n",
        "        vocab_dict - The words dictionary\n",
        "        shuffle - Shuffle the data order\n",
        "    Yield:\n",
        "        inputs - Subset of positive and negative examples\n",
        "        targets - The corresponding labels for the subset\n",
        "        example_weights - An array specifying the importance of each example\n",
        "        \n",
        "    '''     \n",
        "### START GIVEN CODE ###\n",
        "    # make sure the batch size is an even number\n",
        "    # to allow an equal number of positive and negative samples\n",
        "    assert batch_size % 2 == 0\n",
        "    \n",
        "    # Number of positive examples in each batch is half of the batch size\n",
        "    # same with number of negative examples in each batch\n",
        "    n_to_take = batch_size // 2\n",
        "    \n",
        "    # Use pos_index to walk through the data_pos array\n",
        "    # same with neg_index and data_neg\n",
        "    pos_index = 0\n",
        "    neg_index = 0\n",
        "    \n",
        "    len_data_pos = len(data_pos)\n",
        "    len_data_neg = len(data_neg)\n",
        "    \n",
        "    # Get and array with the data indexes\n",
        "    pos_index_lines = list(range(len_data_pos))\n",
        "    neg_index_lines = list(range(len_data_neg))\n",
        "    \n",
        "    # shuffle lines if shuffle is set to True\n",
        "    if shuffle:\n",
        "        rnd.shuffle(pos_index_lines)\n",
        "        rnd.shuffle(neg_index_lines)\n",
        "        \n",
        "    stop = False\n",
        "    \n",
        "    # Loop indefinitely\n",
        "    while not stop:  \n",
        "        \n",
        "        # create a batch with positive and negative examples\n",
        "        batch = []\n",
        "        \n",
        "        # First part: Pack n_to_take positive examples\n",
        "        \n",
        "        # Start from pos_index and increment i up to n_to_take\n",
        "        for i in range(n_to_take):\n",
        "                    \n",
        "            # If the positive index goes past the positive dataset lenght,\n",
        "            if pos_index >= len_data_pos: \n",
        "                \n",
        "                # If loop is set to False, break once we reach the end of the dataset\n",
        "                if not loop:\n",
        "                    stop = True;\n",
        "                    break;\n",
        "                \n",
        "                # If user wants to keep re-using the data, reset the index\n",
        "                pos_index = 0\n",
        "                \n",
        "                if shuffle:\n",
        "                    # Shuffle the index of the positive sample\n",
        "                    rnd.shuffle(pos_index_lines)\n",
        "                    \n",
        "            \n",
        "            text = data_pos[pos_index_lines[pos_index]]\n",
        "            \n",
        "            # convert the tweet into tensors of integers representing the processed words\n",
        "            tensor = text_to_tensor(text, vocab_dict)\n",
        "\n",
        "            tensor = tensor[:1024]\n",
        "            \n",
        "            # append the tensor to the batch list\n",
        "            batch.append(tensor)\n",
        "            \n",
        "            # Increment pos_index by one\n",
        "            pos_index = pos_index + 1\n",
        "\n",
        "### END GIVEN CODE ###\n",
        "            \n",
        "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "\n",
        "        # Second part: Pack n_to_take negative examples\n",
        "    \n",
        "        # Using the same batch list, start from neg_index and increment i up to n_to_take\n",
        "        for i in range(n_to_take):\n",
        "            \n",
        "            # If the negative index goes past the negative dataset length,\n",
        "            if neg_index >= len_data_neg:\n",
        "                \n",
        "                # If loop is set to False, break once we reach the end of the dataset\n",
        "                if not loop:\n",
        "                    stop = True;\n",
        "                    break;\n",
        "                    \n",
        "                # If user wants to keep re-using the data, reset the index\n",
        "                neg_index = 0\n",
        "                \n",
        "                if shuffle:\n",
        "                    # Shuffle the index of the negative sample\n",
        "                    rnd.shuffle(neg_index_lines)\n",
        "            # get the tweet as neg_index\n",
        "            text = data_neg[neg_index_lines[neg_index]]\n",
        "            \n",
        "            # convert the tweet into tensors of integers representing the processed words\n",
        "            tensor = text_to_tensor(text, vocab_dict)\n",
        "\n",
        "            tensor = tensor[:1024]\n",
        "            \n",
        "            # append the tensor to the batch list\n",
        "            batch.append(tensor)\n",
        "            \n",
        "            # Increment neg_index by one\n",
        "            neg_index += 1\n",
        "\n",
        "### END CODE HERE ###        \n",
        "\n",
        "### START GIVEN CODE ###\n",
        "        if stop:\n",
        "            break;\n",
        "\n",
        "        # Update the start index for positive data \n",
        "        # so that it's n_to_take positions after the current pos_index\n",
        "        # pos_index += n_to_take\n",
        "        \n",
        "        # Update the start index for negative data \n",
        "        # so that it's n_to_take positions after the current neg_index\n",
        "        # neg_index += n_to_take\n",
        "        \n",
        "        # Get the max tweet length (the length of the longest tweet) \n",
        "        # (you will pad all shorter tweets to have this length)\n",
        "        max_len = max([len(t) for t in batch]) \n",
        "        \n",
        "        \n",
        "        # Initialize the input_l, which will \n",
        "        # store the padded versions of the tensors\n",
        "        tensor_pad_l = []\n",
        "        # Pad shorter tweets with zeros\n",
        "        for tensor in batch:\n",
        "### END GIVEN CODE ###\n",
        "\n",
        "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "            # Get the number of positions to pad for this tensor so that it will be max_len long\n",
        "            n_pad = max_len - len(tensor)\n",
        "            \n",
        "            # Generate a list of zeros, with length n_pad\n",
        "            pad_l = [0]*n_pad\n",
        "            \n",
        "            # concatenate the tensor and the list of padded zeros\n",
        "            tensor_pad = tensor + pad_l\n",
        "            \n",
        "            # append the padded tensor to the list of padded tensors\n",
        "            tensor_pad_l.append(tensor_pad)\n",
        "\n",
        "        # convert the list of padded tensors to a numpy array\n",
        "        # and store this as the model inputs\n",
        "        inputs = fastnp.array(tensor_pad_l)\n",
        "  \n",
        "        # Generate the list of targets for the positive examples (a list of ones)\n",
        "        # The length is the number of positive examples in the batch\n",
        "        target_pos = [1] * n_to_take\n",
        "        \n",
        "        # Generate the list of targets for the negative examples (a list of zeros)\n",
        "        # The length is the number of negative examples in the batch\n",
        "        target_neg = [0] * n_to_take\n",
        "        \n",
        "        # Concatenate the positve and negative targets\n",
        "        target_l = target_pos + target_neg\n",
        "        \n",
        "        # Convert the target list into a numpy array\n",
        "        targets = fastnp.array(target_l)\n",
        "\n",
        "        # Example weights: Treat all examples equally importantly.\n",
        "        example_weights = [1]*2*n_to_take\n",
        "\n",
        "        inds = list(range(2*n_to_take))\n",
        "        rnd.shuffle(inds)\n",
        "\n",
        "        inputs = inputs[inds]\n",
        "        targets = targets[inds]\n",
        "        \n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "### GIVEN CODE ###\n",
        "        # note we use yield and not return\n",
        "        yield inputs, targets, fastnp.array(example_weights)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCQtIb6G1dHd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "456328f2-6a9d-4ec3-9118-b388805144fc"
      },
      "source": [
        "rnd.seed(30) \n",
        "\n",
        "# Create the training data generator\n",
        "def train_generator(batch_size, shuffle = False):\n",
        "    return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n",
        "\n",
        "# Create the validation data generator\n",
        "def val_generator(batch_size, shuffle = False):\n",
        "    return data_generator(val_pos, val_neg, batch_size, True, Vocab, shuffle)\n",
        "\n",
        "# Create the validation data generator\n",
        "def test_generator(batch_size, shuffle = False):\n",
        "    return data_generator(val_pos, val_neg, batch_size, False, Vocab, shuffle)\n",
        "\n",
        "# Get a batch from the train_generator and inspect.\n",
        "inputs, targets, example_weights = next(train_generator(4, shuffle=True))\n",
        "\n",
        "# this will print a list of 4 tensors padded with zeros\n",
        "print(inputs.shape)\n",
        "print(f'Inputs: {inputs}')\n",
        "print(f'Targets: {targets}')\n",
        "print(f'Example Weights: {example_weights}')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 923)\n",
            "Inputs: [[114664 137765   1252 ...      0      0      0]\n",
            " [ 36214   5486   2245 ...    119    497   1113]\n",
            " [  2530    573   3499 ...      0      0      0]\n",
            " [    70     13   3420 ...      0      0      0]]\n",
            "Targets: [0 1 1 0]\n",
            "Example Weights: [1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB4X8dKc1i7Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ec031119-c3bb-4f04-9056-2df6e00db356"
      },
      "source": [
        "# Create a data generator for training data,\n",
        "# which produces batches of size 4 (for tensors and their respective targets)\n",
        "tmp_data_gen = test_generator(batch_size = 4)\n",
        "\n",
        "# Call the data generator to get one batch and its targets\n",
        "tmp_inputs, tmp_targets, tmp_example_weights = next(tmp_data_gen)\n",
        "\n",
        "print(f\"The inputs shape is {tmp_inputs.shape}\")\n",
        "for i,t in enumerate(tmp_inputs):\n",
        "    print(f\"input tensor: {t}; target {tmp_targets[i]}; example weights {tmp_example_weights[i]}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The inputs shape is (4, 645)\n",
            "input tensor: [96892     2   178  1113  3656   231 39342   493  1874 39342  3520  1792\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0]; target 1; example weights 1\n",
            "input tensor: [   428   1598   2889     44   4014  18968   2188  12556   1801   1363\n",
            "    428   1478  31948  16321   3281   8003  22928   1987   9428   1363\n",
            "  10280  13325     55     36    427    428    560  18089  23411    142\n",
            "    142   5195   7508   5092   9166  26788  16630   9107  31948  16321\n",
            "   1847   2599   7152    351    885   1377 148881   8005    428  16321\n",
            "   4093    104   8464  22928  22928   1987   9428   1987   9428    104\n",
            "   8464    471  22928    104  12304   2599   2610   1030    428    560\n",
            "  18089  20842    106   1135    348    427   1699   1365    497   1782\n",
            "    772    847   1987     99    428    368    586   8761   6624   1354\n",
            "   7514  35038    428    233   3262   1668    795   5806  26900   1962\n",
            "  41766   2090   7308    537  16461   6624   2495   6624    622   1768\n",
            "    622  21708  22927   1875  15965  27521    795    563   7989   3804\n",
            "   5092    427    775    276   6548     77   3773  13513  10280  13325\n",
            "  10280  13325   1339    978    893  17932    284   3956   2292   2286\n",
            "   1550   7708  11492    331   2283  18856  11045   2551   9914   2352\n",
            "   1318   2804   1741   3262    122     37   1798  49593    494    128\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0]; target 1; example weights 1\n",
            "input tensor: [ 23713   2519    755   2943   4920   8232   3160    732   2016  45652\n",
            "    142   4024   6949    138    501  23713    759   5803    732    733\n",
            "    803    732   2620    483   2519   1855   5042    233   1864   6272\n",
            "   4310   8460      5  23713  41343   8491  12613  77204    339   6246\n",
            "   1536    965    142  12613    717  35132  25433  16262   3496    678\n",
            "  23713   1916   5803    803   2777   2620    732  33180    560   5548\n",
            "   3656    384    233  12613   1011   7057  24347   9787    684  17691\n",
            "  25133  12858   2289    233   3768   2448   2986  17100  43860  17048\n",
            "   7970  12593    300   8481    195    130   8498  17048   1940   5622\n",
            "    210    875   3681   4939    564   2052    195    133   1779   1083\n",
            "   5446    132  17048    221    875    933    195    737   8498   2548\n",
            "   3939   6899   3523   7656    732   2435     18   2010    732   1290\n",
            "   3267    737  58240  23713    233  12613   2146    957   1276   3887\n",
            "  21395   2762   2119  12580   5577  43860   2200   1081  18250  17048\n",
            "   3720  17794   2762  17794    281   6277    195    297    281  18141\n",
            "   1829    732    181    976   3319  21369   1173    732   2435   1459\n",
            "  21395   2762   3605   2762  17048    984   1922   7431  19085    732\n",
            "   5814   6272   5349  32967  19857   9693   5885   2959    732   3956\n",
            "   2917    154   8311   3855   4926  14984   5986  17048   2476    882\n",
            "   1399   6272   2356  16252    242    893   2777   2620   3351   2292\n",
            "  12027    233  57085   5286  21395   2762  11511    354  17048    194\n",
            "  57085    233    891    331   3841    322     84    102    233   8498\n",
            "    181    181    497  17048  19085   1276   1321    353   1320   6012\n",
            "   9648    732   2762  12613  23713     25   2762   2915   3071   2701\n",
            "    269    984    729    955  12613   1024  17912   5701   3828   3085\n",
            "    791    503   2010   3357    990    233  23713   6152    656   6008\n",
            "    732    240    233   1397   1307  16705    805  29612   4270  14807\n",
            " 150216  19708  79291   2004   5803  22842 150216   3957  26754  19043\n",
            "   2669   3116    732  79291  51890   2669   3116  10216   2661   3069\n",
            "   4310    593     15   2701   1066   8498   1066  11045  79291    233\n",
            "   6263  21987   2841  43860   2701  10216  52251   1916  25894    796\n",
            "   5803   1066 150216   6480   1258  52394    635   3349   1253  12584\n",
            "   4370  43860    181    771   2669  17636    737    771   5577  25296\n",
            "   2620    240    760   6277  34098    269   2694  79291 150216   9670\n",
            "    339   1252   6622   3084   3988   1589    785   1660   1067  22842\n",
            "   1024   2841   2701  79291    233    239   1785   1940  25133  10075\n",
            "    181 150216    233   2701 126789 108649   8838   2669   3116   5626\n",
            "    751    714   2119  25648  48559   3343  27859   2079   6054  11376\n",
            "    717    505    239  17048     66  43860   5393  48559    635  19061\n",
            "  99215  11575    200   2119   7816   1074   1459    269   9880  17048\n",
            "     74   1922   3841   2289    412   1804  17048    611   1862  14174\n",
            "    320    233  52394      2   4382   2669   5578   4370  43860   5739\n",
            "   5644   2897    130    732   1290   7608    182   3570    737    645\n",
            "    233  40086   7313  11534  25156   6108    399      2      2   4369\n",
            "    497  15231   7449   1290   3116   6272   1084  25117  15929  34270\n",
            "  43755   7313  12724   6108   6277    713    229   2943      2    112\n",
            "   6272    990    739  23908    737    384  11337    233   1812    196\n",
            "  16252    812    429    430   3956    837   8064    787    737   1000\n",
            "   8934   3523  17048   4241   3373  19146   2435   1763  43860   5235\n",
            " 119066   1478    497    732   2530    233  88869  34297    283   5286\n",
            "   2035  43860   1038   1161  59270  17048   2669    729    269    375\n",
            "  43860   1290   2184  17048  59270   1695  34098  15462   4006    771\n",
            "  25296  10217  18762  25117  22652  43860    240  17656   1281   5826\n",
            "    573     85    296   6188    181   7853   8498   9547     25   1066\n",
            "    930   3070    233  43726   2887    269    250   2620  11294  43860\n",
            "   1290   3552   1083   2887    233   1779  17048  42659  59270   5977\n",
            "   1977  43860  16252    295   3986  17048  29680  23179   3675   1287\n",
            "   1705    568   7815  10743     99   2208    233  17048    104  59270\n",
            "   1133   1083   2580  17048   5092   1280   5925   1940  22842   6510\n",
            "     96     85   8181    233  67963  26861   2917    962  34297    283\n",
            "    269   3956  89741  11440  16240   2701     15   2967   2669     80\n",
            "  24438  26861   2502  15945   1011   4043   1589   9670   2701  26861\n",
            "   3522    185   1258   2679  16240    805   5429    200    233    269\n",
            "  43860   1038    537    221    560   5817   4042   1940     99    181\n",
            "  22460   6264  26861    233  20046]; target 0; example weights 1\n",
            "input tensor: [  5792  14992   2127  41265   1996    968  14102   1505  49301   1554\n",
            "   8435   3281   1996  73991   1124   5934   9612  19317   5981    968\n",
            "    210   7155   1713 154634   3755    503   1996   2626  17123   1808\n",
            "   5905  37385  10698    233   1996   1989   2088  15543    340    182\n",
            "  61547   9487   1996    968  13720  14310    360   1273   1996   6372\n",
            "  27848   1996  32226  14102   1505  49301  14310    360  11579   1370\n",
            "    904 154009  36146   5234   1788   1614  14102  83743  14102  96237\n",
            "    933   7527   1996   3631   5993  14102   7300   4083   1183  15761\n",
            "   1852   7527   5566  37223    815   6005    120  14376      4   2445\n",
            "  12598  37223  14376      4  10192    331   3569   1562    130   1799\n",
            "     38   1105 113946   1350   3837 113946  14376      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0]; target 0; example weights 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nbvkXDx1q2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classifier(vocab_size=len(Vocab), embedding_dim=128, output_dim=2, mode='train'):\n",
        "\n",
        "    # embed_layer = tl.Embedding(\n",
        "    #     vocab_size=vocab_size, # Size of the vocabulary\n",
        "    #     d_feature=embedding_dim)  # Embedding dimension\n",
        "    \n",
        "    # # Create a mean layer, to create an \"average\" word embedding\n",
        "    # mean_layer = tl.Mean(axis=1)\n",
        "    \n",
        "    # # Create a dense layer, one unit for each output\n",
        "    # dense_output_layer = tl.Dense(n_units = output_dim)\n",
        "\n",
        "    \n",
        "    # # Create the log softmax layer (no parameters needed)\n",
        "    # log_softmax_layer = tl.LogSoftmax()\n",
        "\n",
        "    model = trax.models.transformer.TransformerEncoder(vocab_size, n_classes=output_dim, d_model=embedding_dim, d_ff=2048, n_layers=1, n_heads=8, dropout=0.1, dropout_shared_axes=None, max_len=1024, mode=mode)\n",
        "    # model = trax.models.research.bert.BERTClassifierHead(2)  \n",
        "    # model = tl.Serial(\n",
        "       \n",
        "    #   embed_layer, # embedding layer\n",
        "    #   mean_layer, # mean layer\n",
        "    #   dense_output_layer, # dense output layer \n",
        "    #   log_softmax_layer # log softmax layer\n",
        "    # )\n",
        "    return model"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUbyVpei2DUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp_model = classifier()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epMnoeAm2Hve",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "outputId": "9757ab94-85eb-4cd1-81fc-1622fb1ab829"
      },
      "source": [
        "print(type(tmp_model))\n",
        "display(tmp_model)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'trax.layers.combinators.Serial'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Serial[\n",
              "  Branch_out2[\n",
              "    [Embedding_154878_128, Dropout, PositionalEncoding]\n",
              "    PaddingMask(0)\n",
              "  ]\n",
              "  Serial_in2_out2[\n",
              "    Branch_in2_out3[\n",
              "      None\n",
              "      Serial_in2_out2[\n",
              "        LayerNorm\n",
              "        Serial_in2_out2[\n",
              "          Dup_out2\n",
              "          Dup_out2\n",
              "          Serial_in4_out2[\n",
              "            Parallel_in3_out3[\n",
              "              Dense_128\n",
              "              Dense_128\n",
              "              Dense_128\n",
              "            ]\n",
              "            PureAttention_in4_out2\n",
              "            Dense_128\n",
              "          ]\n",
              "        ]\n",
              "        Dropout\n",
              "      ]\n",
              "    ]\n",
              "    Add_in2\n",
              "  ]\n",
              "  Serial[\n",
              "    Branch_out2[\n",
              "      None\n",
              "      Serial[\n",
              "        LayerNorm\n",
              "        Dense_2048\n",
              "        Relu\n",
              "        Dropout\n",
              "        Dense_128\n",
              "        Dropout\n",
              "      ]\n",
              "    ]\n",
              "    Add_in2\n",
              "  ]\n",
              "  Select[0]_in2\n",
              "  LayerNorm\n",
              "  Mean\n",
              "  Dense_2\n",
              "  LogSoftmax\n",
              "]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEQU_pLE2JkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from trax.supervised import training\n",
        "\n",
        "batch_size = 16\n",
        "rnd.seed(271)\n",
        "\n",
        "lr_schedule = trax.supervised.lr_schedules.warmup_and_rsqrt_decay(1300,0.001)\n",
        "\n",
        "train_task = training.TrainTask(\n",
        "    labeled_data=train_generator(batch_size=batch_size, shuffle=True),\n",
        "    loss_layer=tl.CrossEntropyLoss(),\n",
        "    optimizer=trax.optimizers.Adam(0.001),\n",
        "    n_steps_per_checkpoint=10,\n",
        "    lr_schedule=lr_schedule\n",
        ")\n",
        "\n",
        "eval_task = training.EvalTask(\n",
        "    labeled_data=val_generator(batch_size=batch_size, shuffle=True),\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        "    n_eval_batches = 32\n",
        ")\n",
        "\n",
        "model = classifier()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQCPlT2m2TbV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70a11236-08ff-4a5c-cc86-539633293d59"
      },
      "source": [
        "import os\n",
        "output_dir = '/content/'\n",
        "output_dir_expand = os.path.expanduser(output_dir)\n",
        "print(output_dir_expand)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kVNMBSN2bMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
        "    '''\n",
        "    Input: \n",
        "        classifier - the model you are building\n",
        "        train_task - Training task\n",
        "        eval_task - Evaluation task\n",
        "        n_steps - the evaluation steps\n",
        "        output_dir - folder to save your files\n",
        "    Output:\n",
        "        trainer -  trax trainer\n",
        "    '''\n",
        "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    training_loop = training.Loop(\n",
        "                                classifier, # The learning model\n",
        "                                train_task, # The training task\n",
        "                                eval_task = eval_task, # The evaluation task\n",
        "                                output_dir = output_dir) # The output directory\n",
        "\n",
        "    training_loop.run(n_steps = n_steps)\n",
        "### END CODE HERE ###\n",
        "\n",
        "    # Return the training_loop, since it has the model.\n",
        "    return training_loop"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoE_LupM2hbz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e67f3c44-1114-49f4-fd6e-15c52fe05512"
      },
      "source": [
        "training_loop = train_model(model, train_task, eval_task, 3900, output_dir_expand)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step      1: train CrossEntropyLoss |  0.75910896\n",
            "Step      1: eval  CrossEntropyLoss |  0.77631182\n",
            "Step      1: eval          Accuracy |  0.50000000\n",
            "Step     10: train CrossEntropyLoss |  0.76132399\n",
            "Step     10: eval  CrossEntropyLoss |  0.72381587\n",
            "Step     10: eval          Accuracy |  0.49804688\n",
            "Step     20: train CrossEntropyLoss |  0.68720669\n",
            "Step     20: eval  CrossEntropyLoss |  0.65392923\n",
            "Step     20: eval          Accuracy |  0.62695312\n",
            "Step     30: train CrossEntropyLoss |  0.68213230\n",
            "Step     30: eval  CrossEntropyLoss |  0.66771900\n",
            "Step     30: eval          Accuracy |  0.58984375\n",
            "Step     40: train CrossEntropyLoss |  0.66077757\n",
            "Step     40: eval  CrossEntropyLoss |  0.64561101\n",
            "Step     40: eval          Accuracy |  0.64843750\n",
            "Step     50: train CrossEntropyLoss |  0.68251908\n",
            "Step     50: eval  CrossEntropyLoss |  0.63639805\n",
            "Step     50: eval          Accuracy |  0.66992188\n",
            "Step     60: train CrossEntropyLoss |  0.62826985\n",
            "Step     60: eval  CrossEntropyLoss |  0.62661697\n",
            "Step     60: eval          Accuracy |  0.64257812\n",
            "Step     70: train CrossEntropyLoss |  0.63991779\n",
            "Step     70: eval  CrossEntropyLoss |  0.60923217\n",
            "Step     70: eval          Accuracy |  0.65429688\n",
            "Step     80: train CrossEntropyLoss |  0.65861750\n",
            "Step     80: eval  CrossEntropyLoss |  0.64225132\n",
            "Step     80: eval          Accuracy |  0.64648438\n",
            "Step     90: train CrossEntropyLoss |  0.61719620\n",
            "Step     90: eval  CrossEntropyLoss |  0.63719952\n",
            "Step     90: eval          Accuracy |  0.63085938\n",
            "Step    100: train CrossEntropyLoss |  0.61471325\n",
            "Step    100: eval  CrossEntropyLoss |  0.60677325\n",
            "Step    100: eval          Accuracy |  0.68750000\n",
            "Step    110: train CrossEntropyLoss |  0.58577520\n",
            "Step    110: eval  CrossEntropyLoss |  0.60838270\n",
            "Step    110: eval          Accuracy |  0.65429688\n",
            "Step    120: train CrossEntropyLoss |  0.61845732\n",
            "Step    120: eval  CrossEntropyLoss |  0.60816367\n",
            "Step    120: eval          Accuracy |  0.65234375\n",
            "Step    130: train CrossEntropyLoss |  0.66521227\n",
            "Step    130: eval  CrossEntropyLoss |  0.60484266\n",
            "Step    130: eval          Accuracy |  0.66015625\n",
            "Step    140: train CrossEntropyLoss |  0.59528774\n",
            "Step    140: eval  CrossEntropyLoss |  0.59193041\n",
            "Step    140: eval          Accuracy |  0.66796875\n",
            "Step    150: train CrossEntropyLoss |  0.63441366\n",
            "Step    150: eval  CrossEntropyLoss |  0.58228311\n",
            "Step    150: eval          Accuracy |  0.66210938\n",
            "Step    160: train CrossEntropyLoss |  0.55260617\n",
            "Step    160: eval  CrossEntropyLoss |  0.57209234\n",
            "Step    160: eval          Accuracy |  0.70117188\n",
            "Step    170: train CrossEntropyLoss |  0.55030650\n",
            "Step    170: eval  CrossEntropyLoss |  0.54942648\n",
            "Step    170: eval          Accuracy |  0.70117188\n",
            "Step    180: train CrossEntropyLoss |  0.57868701\n",
            "Step    180: eval  CrossEntropyLoss |  0.54427419\n",
            "Step    180: eval          Accuracy |  0.73632812\n",
            "Step    190: train CrossEntropyLoss |  0.56757528\n",
            "Step    190: eval  CrossEntropyLoss |  0.54280770\n",
            "Step    190: eval          Accuracy |  0.68554688\n",
            "Step    200: train CrossEntropyLoss |  0.58654565\n",
            "Step    200: eval  CrossEntropyLoss |  0.53296658\n",
            "Step    200: eval          Accuracy |  0.75781250\n",
            "Step    210: train CrossEntropyLoss |  0.48669073\n",
            "Step    210: eval  CrossEntropyLoss |  0.51352125\n",
            "Step    210: eval          Accuracy |  0.70898438\n",
            "Step    220: train CrossEntropyLoss |  0.48145461\n",
            "Step    220: eval  CrossEntropyLoss |  0.48369599\n",
            "Step    220: eval          Accuracy |  0.78320312\n",
            "Step    230: train CrossEntropyLoss |  0.43440223\n",
            "Step    230: eval  CrossEntropyLoss |  0.54106798\n",
            "Step    230: eval          Accuracy |  0.75976562\n",
            "Step    240: train CrossEntropyLoss |  0.48232481\n",
            "Step    240: eval  CrossEntropyLoss |  0.47674756\n",
            "Step    240: eval          Accuracy |  0.78906250\n",
            "Step    250: train CrossEntropyLoss |  0.47325668\n",
            "Step    250: eval  CrossEntropyLoss |  0.51796837\n",
            "Step    250: eval          Accuracy |  0.76171875\n",
            "Step    260: train CrossEntropyLoss |  0.47475177\n",
            "Step    260: eval  CrossEntropyLoss |  0.48366759\n",
            "Step    260: eval          Accuracy |  0.76562500\n",
            "Step    270: train CrossEntropyLoss |  0.51857692\n",
            "Step    270: eval  CrossEntropyLoss |  0.51220635\n",
            "Step    270: eval          Accuracy |  0.76757812\n",
            "Step    280: train CrossEntropyLoss |  0.44908366\n",
            "Step    280: eval  CrossEntropyLoss |  0.44375886\n",
            "Step    280: eval          Accuracy |  0.80859375\n",
            "Step    290: train CrossEntropyLoss |  0.43045759\n",
            "Step    290: eval  CrossEntropyLoss |  0.42310912\n",
            "Step    290: eval          Accuracy |  0.80859375\n",
            "Step    300: train CrossEntropyLoss |  0.42805836\n",
            "Step    300: eval  CrossEntropyLoss |  0.53998689\n",
            "Step    300: eval          Accuracy |  0.72656250\n",
            "Step    310: train CrossEntropyLoss |  0.48151827\n",
            "Step    310: eval  CrossEntropyLoss |  0.49489807\n",
            "Step    310: eval          Accuracy |  0.78710938\n",
            "Step    320: train CrossEntropyLoss |  0.50513214\n",
            "Step    320: eval  CrossEntropyLoss |  0.48110376\n",
            "Step    320: eval          Accuracy |  0.79687500\n",
            "Step    330: train CrossEntropyLoss |  0.45769963\n",
            "Step    330: eval  CrossEntropyLoss |  0.39018707\n",
            "Step    330: eval          Accuracy |  0.83789062\n",
            "Step    340: train CrossEntropyLoss |  0.41585714\n",
            "Step    340: eval  CrossEntropyLoss |  0.33928372\n",
            "Step    340: eval          Accuracy |  0.84765625\n",
            "Step    350: train CrossEntropyLoss |  0.43255815\n",
            "Step    350: eval  CrossEntropyLoss |  0.44821960\n",
            "Step    350: eval          Accuracy |  0.80273438\n",
            "Step    360: train CrossEntropyLoss |  0.43853125\n",
            "Step    360: eval  CrossEntropyLoss |  0.39375309\n",
            "Step    360: eval          Accuracy |  0.81445312\n",
            "Step    370: train CrossEntropyLoss |  0.35076949\n",
            "Step    370: eval  CrossEntropyLoss |  0.32215112\n",
            "Step    370: eval          Accuracy |  0.86914062\n",
            "Step    380: train CrossEntropyLoss |  0.36879718\n",
            "Step    380: eval  CrossEntropyLoss |  0.38122298\n",
            "Step    380: eval          Accuracy |  0.83007812\n",
            "Step    390: train CrossEntropyLoss |  0.46079689\n",
            "Step    390: eval  CrossEntropyLoss |  0.49309501\n",
            "Step    390: eval          Accuracy |  0.76757812\n",
            "Step    400: train CrossEntropyLoss |  0.47910950\n",
            "Step    400: eval  CrossEntropyLoss |  0.41044166\n",
            "Step    400: eval          Accuracy |  0.83203125\n",
            "Step    410: train CrossEntropyLoss |  0.40278193\n",
            "Step    410: eval  CrossEntropyLoss |  0.36388456\n",
            "Step    410: eval          Accuracy |  0.83593750\n",
            "Step    420: train CrossEntropyLoss |  0.39316857\n",
            "Step    420: eval  CrossEntropyLoss |  0.39485304\n",
            "Step    420: eval          Accuracy |  0.82226562\n",
            "Step    430: train CrossEntropyLoss |  0.34833869\n",
            "Step    430: eval  CrossEntropyLoss |  0.32874038\n",
            "Step    430: eval          Accuracy |  0.85937500\n",
            "Step    440: train CrossEntropyLoss |  0.42471692\n",
            "Step    440: eval  CrossEntropyLoss |  0.34842832\n",
            "Step    440: eval          Accuracy |  0.83984375\n",
            "Step    450: train CrossEntropyLoss |  0.41238871\n",
            "Step    450: eval  CrossEntropyLoss |  0.31943317\n",
            "Step    450: eval          Accuracy |  0.86523438\n",
            "Step    460: train CrossEntropyLoss |  0.36055577\n",
            "Step    460: eval  CrossEntropyLoss |  0.38273228\n",
            "Step    460: eval          Accuracy |  0.81640625\n",
            "Step    470: train CrossEntropyLoss |  0.37350640\n",
            "Step    470: eval  CrossEntropyLoss |  0.36439318\n",
            "Step    470: eval          Accuracy |  0.82617188\n",
            "Step    480: train CrossEntropyLoss |  0.35211760\n",
            "Step    480: eval  CrossEntropyLoss |  0.27429686\n",
            "Step    480: eval          Accuracy |  0.89648438\n",
            "Step    490: train CrossEntropyLoss |  0.32358041\n",
            "Step    490: eval  CrossEntropyLoss |  0.31566658\n",
            "Step    490: eval          Accuracy |  0.86914062\n",
            "Step    500: train CrossEntropyLoss |  0.22886460\n",
            "Step    500: eval  CrossEntropyLoss |  0.29085969\n",
            "Step    500: eval          Accuracy |  0.89843750\n",
            "Step    510: train CrossEntropyLoss |  0.33073118\n",
            "Step    510: eval  CrossEntropyLoss |  0.39670701\n",
            "Step    510: eval          Accuracy |  0.82031250\n",
            "Step    520: train CrossEntropyLoss |  0.24694160\n",
            "Step    520: eval  CrossEntropyLoss |  0.28329622\n",
            "Step    520: eval          Accuracy |  0.85937500\n",
            "Step    530: train CrossEntropyLoss |  0.30181149\n",
            "Step    530: eval  CrossEntropyLoss |  0.46515626\n",
            "Step    530: eval          Accuracy |  0.80859375\n",
            "Step    540: train CrossEntropyLoss |  0.31675407\n",
            "Step    540: eval  CrossEntropyLoss |  0.26917201\n",
            "Step    540: eval          Accuracy |  0.88476562\n",
            "Step    550: train CrossEntropyLoss |  0.29336241\n",
            "Step    550: eval  CrossEntropyLoss |  0.34317140\n",
            "Step    550: eval          Accuracy |  0.85156250\n",
            "Step    560: train CrossEntropyLoss |  0.25872827\n",
            "Step    560: eval  CrossEntropyLoss |  0.40333672\n",
            "Step    560: eval          Accuracy |  0.82421875\n",
            "Step    570: train CrossEntropyLoss |  0.32483321\n",
            "Step    570: eval  CrossEntropyLoss |  0.25011627\n",
            "Step    570: eval          Accuracy |  0.88671875\n",
            "Step    580: train CrossEntropyLoss |  0.35492668\n",
            "Step    580: eval  CrossEntropyLoss |  0.37065070\n",
            "Step    580: eval          Accuracy |  0.82031250\n",
            "Step    590: train CrossEntropyLoss |  0.32044506\n",
            "Step    590: eval  CrossEntropyLoss |  0.27861735\n",
            "Step    590: eval          Accuracy |  0.87890625\n",
            "Step    600: train CrossEntropyLoss |  0.22422600\n",
            "Step    600: eval  CrossEntropyLoss |  0.28825507\n",
            "Step    600: eval          Accuracy |  0.87695312\n",
            "Step    610: train CrossEntropyLoss |  0.24881199\n",
            "Step    610: eval  CrossEntropyLoss |  0.28083360\n",
            "Step    610: eval          Accuracy |  0.87890625\n",
            "Step    620: train CrossEntropyLoss |  0.33704314\n",
            "Step    620: eval  CrossEntropyLoss |  0.28394209\n",
            "Step    620: eval          Accuracy |  0.89843750\n",
            "Step    630: train CrossEntropyLoss |  0.30205226\n",
            "Step    630: eval  CrossEntropyLoss |  0.25563312\n",
            "Step    630: eval          Accuracy |  0.88476562\n",
            "Step    640: train CrossEntropyLoss |  0.38397142\n",
            "Step    640: eval  CrossEntropyLoss |  0.31735060\n",
            "Step    640: eval          Accuracy |  0.86132812\n",
            "Step    650: train CrossEntropyLoss |  0.28947166\n",
            "Step    650: eval  CrossEntropyLoss |  0.27591524\n",
            "Step    650: eval          Accuracy |  0.88867188\n",
            "Step    660: train CrossEntropyLoss |  0.33900061\n",
            "Step    660: eval  CrossEntropyLoss |  0.28225600\n",
            "Step    660: eval          Accuracy |  0.88085938\n",
            "Step    670: train CrossEntropyLoss |  0.27765784\n",
            "Step    670: eval  CrossEntropyLoss |  0.32087210\n",
            "Step    670: eval          Accuracy |  0.87304688\n",
            "Step    680: train CrossEntropyLoss |  0.29526621\n",
            "Step    680: eval  CrossEntropyLoss |  0.21526774\n",
            "Step    680: eval          Accuracy |  0.91796875\n",
            "Step    690: train CrossEntropyLoss |  0.30523279\n",
            "Step    690: eval  CrossEntropyLoss |  0.31989726\n",
            "Step    690: eval          Accuracy |  0.84765625\n",
            "Step    700: train CrossEntropyLoss |  0.31553110\n",
            "Step    700: eval  CrossEntropyLoss |  0.31904831\n",
            "Step    700: eval          Accuracy |  0.85937500\n",
            "Step    710: train CrossEntropyLoss |  0.24459267\n",
            "Step    710: eval  CrossEntropyLoss |  0.25524032\n",
            "Step    710: eval          Accuracy |  0.89062500\n",
            "Step    720: train CrossEntropyLoss |  0.26007146\n",
            "Step    720: eval  CrossEntropyLoss |  0.34738537\n",
            "Step    720: eval          Accuracy |  0.85156250\n",
            "Step    730: train CrossEntropyLoss |  0.29108426\n",
            "Step    730: eval  CrossEntropyLoss |  0.30371504\n",
            "Step    730: eval          Accuracy |  0.87304688\n",
            "Step    740: train CrossEntropyLoss |  0.31181788\n",
            "Step    740: eval  CrossEntropyLoss |  0.28161841\n",
            "Step    740: eval          Accuracy |  0.87500000\n",
            "Step    750: train CrossEntropyLoss |  0.27894434\n",
            "Step    750: eval  CrossEntropyLoss |  0.24688876\n",
            "Step    750: eval          Accuracy |  0.89648438\n",
            "Step    760: train CrossEntropyLoss |  0.30139375\n",
            "Step    760: eval  CrossEntropyLoss |  0.23354695\n",
            "Step    760: eval          Accuracy |  0.91210938\n",
            "Step    770: train CrossEntropyLoss |  0.23272613\n",
            "Step    770: eval  CrossEntropyLoss |  0.28715931\n",
            "Step    770: eval          Accuracy |  0.88281250\n",
            "Step    780: train CrossEntropyLoss |  0.25267574\n",
            "Step    780: eval  CrossEntropyLoss |  0.23631274\n",
            "Step    780: eval          Accuracy |  0.90429688\n",
            "Step    790: train CrossEntropyLoss |  0.30773327\n",
            "Step    790: eval  CrossEntropyLoss |  0.23631404\n",
            "Step    790: eval          Accuracy |  0.89453125\n",
            "Step    800: train CrossEntropyLoss |  0.24081576\n",
            "Step    800: eval  CrossEntropyLoss |  0.24485864\n",
            "Step    800: eval          Accuracy |  0.91015625\n",
            "Step    810: train CrossEntropyLoss |  0.20325451\n",
            "Step    810: eval  CrossEntropyLoss |  0.23773805\n",
            "Step    810: eval          Accuracy |  0.89062500\n",
            "Step    820: train CrossEntropyLoss |  0.21597481\n",
            "Step    820: eval  CrossEntropyLoss |  0.36026218\n",
            "Step    820: eval          Accuracy |  0.85351562\n",
            "Step    830: train CrossEntropyLoss |  0.24574278\n",
            "Step    830: eval  CrossEntropyLoss |  0.24054153\n",
            "Step    830: eval          Accuracy |  0.89843750\n",
            "Step    840: train CrossEntropyLoss |  0.24163845\n",
            "Step    840: eval  CrossEntropyLoss |  0.25354594\n",
            "Step    840: eval          Accuracy |  0.88476562\n",
            "Step    850: train CrossEntropyLoss |  0.31947485\n",
            "Step    850: eval  CrossEntropyLoss |  0.29384248\n",
            "Step    850: eval          Accuracy |  0.88476562\n",
            "Step    860: train CrossEntropyLoss |  0.19773750\n",
            "Step    860: eval  CrossEntropyLoss |  0.25262194\n",
            "Step    860: eval          Accuracy |  0.89453125\n",
            "Step    870: train CrossEntropyLoss |  0.23573196\n",
            "Step    870: eval  CrossEntropyLoss |  0.26340707\n",
            "Step    870: eval          Accuracy |  0.87890625\n",
            "Step    880: train CrossEntropyLoss |  0.22344008\n",
            "Step    880: eval  CrossEntropyLoss |  0.25485701\n",
            "Step    880: eval          Accuracy |  0.90429688\n",
            "Step    890: train CrossEntropyLoss |  0.23394716\n",
            "Step    890: eval  CrossEntropyLoss |  0.27370687\n",
            "Step    890: eval          Accuracy |  0.87695312\n",
            "Step    900: train CrossEntropyLoss |  0.25077045\n",
            "Step    900: eval  CrossEntropyLoss |  0.28854956\n",
            "Step    900: eval          Accuracy |  0.88671875\n",
            "Step    910: train CrossEntropyLoss |  0.38608640\n",
            "Step    910: eval  CrossEntropyLoss |  0.27514698\n",
            "Step    910: eval          Accuracy |  0.88867188\n",
            "Step    920: train CrossEntropyLoss |  0.21907373\n",
            "Step    920: eval  CrossEntropyLoss |  0.23117062\n",
            "Step    920: eval          Accuracy |  0.92382812\n",
            "Step    930: train CrossEntropyLoss |  0.22439483\n",
            "Step    930: eval  CrossEntropyLoss |  0.20432760\n",
            "Step    930: eval          Accuracy |  0.91796875\n",
            "Step    940: train CrossEntropyLoss |  0.21373069\n",
            "Step    940: eval  CrossEntropyLoss |  0.25339378\n",
            "Step    940: eval          Accuracy |  0.90429688\n",
            "Step    950: train CrossEntropyLoss |  0.28390273\n",
            "Step    950: eval  CrossEntropyLoss |  0.21109422\n",
            "Step    950: eval          Accuracy |  0.90625000\n",
            "Step    960: train CrossEntropyLoss |  0.21733034\n",
            "Step    960: eval  CrossEntropyLoss |  0.23287787\n",
            "Step    960: eval          Accuracy |  0.89062500\n",
            "Step    970: train CrossEntropyLoss |  0.20314239\n",
            "Step    970: eval  CrossEntropyLoss |  0.27034451\n",
            "Step    970: eval          Accuracy |  0.90039062\n",
            "Step    980: train CrossEntropyLoss |  0.22062218\n",
            "Step    980: eval  CrossEntropyLoss |  0.25375829\n",
            "Step    980: eval          Accuracy |  0.89453125\n",
            "Step    990: train CrossEntropyLoss |  0.26672477\n",
            "Step    990: eval  CrossEntropyLoss |  0.24952984\n",
            "Step    990: eval          Accuracy |  0.90234375\n",
            "Step   1000: train CrossEntropyLoss |  0.19536309\n",
            "Step   1000: eval  CrossEntropyLoss |  0.22634103\n",
            "Step   1000: eval          Accuracy |  0.91015625\n",
            "Step   1010: train CrossEntropyLoss |  0.33126995\n",
            "Step   1010: eval  CrossEntropyLoss |  0.20168853\n",
            "Step   1010: eval          Accuracy |  0.91406250\n",
            "Step   1020: train CrossEntropyLoss |  0.20728652\n",
            "Step   1020: eval  CrossEntropyLoss |  0.22695890\n",
            "Step   1020: eval          Accuracy |  0.90625000\n",
            "Step   1030: train CrossEntropyLoss |  0.21952061\n",
            "Step   1030: eval  CrossEntropyLoss |  0.25771027\n",
            "Step   1030: eval          Accuracy |  0.89648438\n",
            "Step   1040: train CrossEntropyLoss |  0.22563183\n",
            "Step   1040: eval  CrossEntropyLoss |  0.20972711\n",
            "Step   1040: eval          Accuracy |  0.90625000\n",
            "Step   1050: train CrossEntropyLoss |  0.30127594\n",
            "Step   1050: eval  CrossEntropyLoss |  0.29430290\n",
            "Step   1050: eval          Accuracy |  0.86132812\n",
            "Step   1060: train CrossEntropyLoss |  0.30806789\n",
            "Step   1060: eval  CrossEntropyLoss |  0.24854380\n",
            "Step   1060: eval          Accuracy |  0.91210938\n",
            "Step   1070: train CrossEntropyLoss |  0.18526825\n",
            "Step   1070: eval  CrossEntropyLoss |  0.18191929\n",
            "Step   1070: eval          Accuracy |  0.92773438\n",
            "Step   1080: train CrossEntropyLoss |  0.20880342\n",
            "Step   1080: eval  CrossEntropyLoss |  0.23991494\n",
            "Step   1080: eval          Accuracy |  0.89257812\n",
            "Step   1090: train CrossEntropyLoss |  0.24023369\n",
            "Step   1090: eval  CrossEntropyLoss |  0.23438438\n",
            "Step   1090: eval          Accuracy |  0.89453125\n",
            "Step   1100: train CrossEntropyLoss |  0.20188859\n",
            "Step   1100: eval  CrossEntropyLoss |  0.23203799\n",
            "Step   1100: eval          Accuracy |  0.89453125\n",
            "Step   1110: train CrossEntropyLoss |  0.13231991\n",
            "Step   1110: eval  CrossEntropyLoss |  0.24455232\n",
            "Step   1110: eval          Accuracy |  0.89648438\n",
            "Step   1120: train CrossEntropyLoss |  0.20686877\n",
            "Step   1120: eval  CrossEntropyLoss |  0.18439978\n",
            "Step   1120: eval          Accuracy |  0.93359375\n",
            "Step   1130: train CrossEntropyLoss |  0.30578277\n",
            "Step   1130: eval  CrossEntropyLoss |  0.20879094\n",
            "Step   1130: eval          Accuracy |  0.91406250\n",
            "Step   1140: train CrossEntropyLoss |  0.19448583\n",
            "Step   1140: eval  CrossEntropyLoss |  0.23551531\n",
            "Step   1140: eval          Accuracy |  0.90234375\n",
            "Step   1150: train CrossEntropyLoss |  0.20060506\n",
            "Step   1150: eval  CrossEntropyLoss |  0.23290619\n",
            "Step   1150: eval          Accuracy |  0.91210938\n",
            "Step   1160: train CrossEntropyLoss |  0.13377315\n",
            "Step   1160: eval  CrossEntropyLoss |  0.20644039\n",
            "Step   1160: eval          Accuracy |  0.91601562\n",
            "Step   1170: train CrossEntropyLoss |  0.16389391\n",
            "Step   1170: eval  CrossEntropyLoss |  0.20917243\n",
            "Step   1170: eval          Accuracy |  0.91015625\n",
            "Step   1180: train CrossEntropyLoss |  0.20721804\n",
            "Step   1180: eval  CrossEntropyLoss |  0.27785707\n",
            "Step   1180: eval          Accuracy |  0.89062500\n",
            "Step   1190: train CrossEntropyLoss |  0.23498674\n",
            "Step   1190: eval  CrossEntropyLoss |  0.25530903\n",
            "Step   1190: eval          Accuracy |  0.90234375\n",
            "Step   1200: train CrossEntropyLoss |  0.22900955\n",
            "Step   1200: eval  CrossEntropyLoss |  0.20953721\n",
            "Step   1200: eval          Accuracy |  0.92382812\n",
            "Step   1210: train CrossEntropyLoss |  0.17923141\n",
            "Step   1210: eval  CrossEntropyLoss |  0.27384273\n",
            "Step   1210: eval          Accuracy |  0.88867188\n",
            "Step   1220: train CrossEntropyLoss |  0.23032251\n",
            "Step   1220: eval  CrossEntropyLoss |  0.26795933\n",
            "Step   1220: eval          Accuracy |  0.87304688\n",
            "Step   1230: train CrossEntropyLoss |  0.13872756\n",
            "Step   1230: eval  CrossEntropyLoss |  0.17618789\n",
            "Step   1230: eval          Accuracy |  0.92578125\n",
            "Step   1240: train CrossEntropyLoss |  0.15564452\n",
            "Step   1240: eval  CrossEntropyLoss |  0.19024768\n",
            "Step   1240: eval          Accuracy |  0.92773438\n",
            "Step   1250: train CrossEntropyLoss |  0.22402112\n",
            "Step   1250: eval  CrossEntropyLoss |  0.24930958\n",
            "Step   1250: eval          Accuracy |  0.89843750\n",
            "Step   1260: train CrossEntropyLoss |  0.12871429\n",
            "Step   1260: eval  CrossEntropyLoss |  0.21663138\n",
            "Step   1260: eval          Accuracy |  0.91406250\n",
            "Step   1270: train CrossEntropyLoss |  0.16474727\n",
            "Step   1270: eval  CrossEntropyLoss |  0.21743621\n",
            "Step   1270: eval          Accuracy |  0.92773438\n",
            "Step   1280: train CrossEntropyLoss |  0.18731023\n",
            "Step   1280: eval  CrossEntropyLoss |  0.19446038\n",
            "Step   1280: eval          Accuracy |  0.92382812\n",
            "Step   1290: train CrossEntropyLoss |  0.18830305\n",
            "Step   1290: eval  CrossEntropyLoss |  0.22393188\n",
            "Step   1290: eval          Accuracy |  0.91210938\n",
            "Step   1300: train CrossEntropyLoss |  0.19574611\n",
            "Step   1300: eval  CrossEntropyLoss |  0.27256244\n",
            "Step   1300: eval          Accuracy |  0.87500000\n",
            "Step   1310: train CrossEntropyLoss |  0.30565044\n",
            "Step   1310: eval  CrossEntropyLoss |  0.24727304\n",
            "Step   1310: eval          Accuracy |  0.89257812\n",
            "Step   1320: train CrossEntropyLoss |  0.24287100\n",
            "Step   1320: eval  CrossEntropyLoss |  0.23437664\n",
            "Step   1320: eval          Accuracy |  0.90625000\n",
            "Step   1330: train CrossEntropyLoss |  0.23997982\n",
            "Step   1330: eval  CrossEntropyLoss |  0.27043568\n",
            "Step   1330: eval          Accuracy |  0.88867188\n",
            "Step   1340: train CrossEntropyLoss |  0.18524031\n",
            "Step   1340: eval  CrossEntropyLoss |  0.29177263\n",
            "Step   1340: eval          Accuracy |  0.88867188\n",
            "Step   1350: train CrossEntropyLoss |  0.26577225\n",
            "Step   1350: eval  CrossEntropyLoss |  0.18578995\n",
            "Step   1350: eval          Accuracy |  0.93164062\n",
            "Step   1360: train CrossEntropyLoss |  0.10969158\n",
            "Step   1360: eval  CrossEntropyLoss |  0.15476064\n",
            "Step   1360: eval          Accuracy |  0.93164062\n",
            "Step   1370: train CrossEntropyLoss |  0.17961378\n",
            "Step   1370: eval  CrossEntropyLoss |  0.19161352\n",
            "Step   1370: eval          Accuracy |  0.93359375\n",
            "Step   1380: train CrossEntropyLoss |  0.14955109\n",
            "Step   1380: eval  CrossEntropyLoss |  0.34549413\n",
            "Step   1380: eval          Accuracy |  0.86718750\n",
            "Step   1390: train CrossEntropyLoss |  0.15731792\n",
            "Step   1390: eval  CrossEntropyLoss |  0.19490214\n",
            "Step   1390: eval          Accuracy |  0.92578125\n",
            "Step   1400: train CrossEntropyLoss |  0.32815239\n",
            "Step   1400: eval  CrossEntropyLoss |  0.25460830\n",
            "Step   1400: eval          Accuracy |  0.90039062\n",
            "Step   1410: train CrossEntropyLoss |  0.17613226\n",
            "Step   1410: eval  CrossEntropyLoss |  0.21436933\n",
            "Step   1410: eval          Accuracy |  0.90429688\n",
            "Step   1420: train CrossEntropyLoss |  0.17737579\n",
            "Step   1420: eval  CrossEntropyLoss |  0.22143822\n",
            "Step   1420: eval          Accuracy |  0.91210938\n",
            "Step   1430: train CrossEntropyLoss |  0.20718622\n",
            "Step   1430: eval  CrossEntropyLoss |  0.20527965\n",
            "Step   1430: eval          Accuracy |  0.92968750\n",
            "Step   1440: train CrossEntropyLoss |  0.22475903\n",
            "Step   1440: eval  CrossEntropyLoss |  0.18198850\n",
            "Step   1440: eval          Accuracy |  0.92968750\n",
            "Step   1450: train CrossEntropyLoss |  0.18299083\n",
            "Step   1450: eval  CrossEntropyLoss |  0.20685017\n",
            "Step   1450: eval          Accuracy |  0.91796875\n",
            "Step   1460: train CrossEntropyLoss |  0.15488127\n",
            "Step   1460: eval  CrossEntropyLoss |  0.22076273\n",
            "Step   1460: eval          Accuracy |  0.91015625\n",
            "Step   1470: train CrossEntropyLoss |  0.19309084\n",
            "Step   1470: eval  CrossEntropyLoss |  0.20292855\n",
            "Step   1470: eval          Accuracy |  0.92578125\n",
            "Step   1480: train CrossEntropyLoss |  0.15613480\n",
            "Step   1480: eval  CrossEntropyLoss |  0.23507321\n",
            "Step   1480: eval          Accuracy |  0.90429688\n",
            "Step   1490: train CrossEntropyLoss |  0.23033765\n",
            "Step   1490: eval  CrossEntropyLoss |  0.20528724\n",
            "Step   1490: eval          Accuracy |  0.92382812\n",
            "Step   1500: train CrossEntropyLoss |  0.22404838\n",
            "Step   1500: eval  CrossEntropyLoss |  0.18787379\n",
            "Step   1500: eval          Accuracy |  0.92382812\n",
            "Step   1510: train CrossEntropyLoss |  0.16234156\n",
            "Step   1510: eval  CrossEntropyLoss |  0.19143936\n",
            "Step   1510: eval          Accuracy |  0.92382812\n",
            "Step   1520: train CrossEntropyLoss |  0.17532212\n",
            "Step   1520: eval  CrossEntropyLoss |  0.18444868\n",
            "Step   1520: eval          Accuracy |  0.92773438\n",
            "Step   1530: train CrossEntropyLoss |  0.09963735\n",
            "Step   1530: eval  CrossEntropyLoss |  0.21667011\n",
            "Step   1530: eval          Accuracy |  0.91406250\n",
            "Step   1540: train CrossEntropyLoss |  0.21710585\n",
            "Step   1540: eval  CrossEntropyLoss |  0.18844095\n",
            "Step   1540: eval          Accuracy |  0.92968750\n",
            "Step   1550: train CrossEntropyLoss |  0.20183592\n",
            "Step   1550: eval  CrossEntropyLoss |  0.24395815\n",
            "Step   1550: eval          Accuracy |  0.89257812\n",
            "Step   1560: train CrossEntropyLoss |  0.12378701\n",
            "Step   1560: eval  CrossEntropyLoss |  0.16805937\n",
            "Step   1560: eval          Accuracy |  0.92773438\n",
            "Step   1570: train CrossEntropyLoss |  0.23866157\n",
            "Step   1570: eval  CrossEntropyLoss |  0.19283063\n",
            "Step   1570: eval          Accuracy |  0.92187500\n",
            "Step   1580: train CrossEntropyLoss |  0.19744919\n",
            "Step   1580: eval  CrossEntropyLoss |  0.27149269\n",
            "Step   1580: eval          Accuracy |  0.89453125\n",
            "Step   1590: train CrossEntropyLoss |  0.16464640\n",
            "Step   1590: eval  CrossEntropyLoss |  0.20386926\n",
            "Step   1590: eval          Accuracy |  0.91406250\n",
            "Step   1600: train CrossEntropyLoss |  0.22625838\n",
            "Step   1600: eval  CrossEntropyLoss |  0.15809343\n",
            "Step   1600: eval          Accuracy |  0.92773438\n",
            "Step   1610: train CrossEntropyLoss |  0.23460737\n",
            "Step   1610: eval  CrossEntropyLoss |  0.21946405\n",
            "Step   1610: eval          Accuracy |  0.90820312\n",
            "Step   1620: train CrossEntropyLoss |  0.16383773\n",
            "Step   1620: eval  CrossEntropyLoss |  0.21541998\n",
            "Step   1620: eval          Accuracy |  0.91015625\n",
            "Step   1630: train CrossEntropyLoss |  0.20094573\n",
            "Step   1630: eval  CrossEntropyLoss |  0.18882977\n",
            "Step   1630: eval          Accuracy |  0.91210938\n",
            "Step   1640: train CrossEntropyLoss |  0.10854516\n",
            "Step   1640: eval  CrossEntropyLoss |  0.15504474\n",
            "Step   1640: eval          Accuracy |  0.94531250\n",
            "Step   1650: train CrossEntropyLoss |  0.16383889\n",
            "Step   1650: eval  CrossEntropyLoss |  0.24345769\n",
            "Step   1650: eval          Accuracy |  0.90820312\n",
            "Step   1660: train CrossEntropyLoss |  0.19775335\n",
            "Step   1660: eval  CrossEntropyLoss |  0.17968593\n",
            "Step   1660: eval          Accuracy |  0.92382812\n",
            "Step   1670: train CrossEntropyLoss |  0.16218062\n",
            "Step   1670: eval  CrossEntropyLoss |  0.19670765\n",
            "Step   1670: eval          Accuracy |  0.91601562\n",
            "Step   1680: train CrossEntropyLoss |  0.21355009\n",
            "Step   1680: eval  CrossEntropyLoss |  0.20949547\n",
            "Step   1680: eval          Accuracy |  0.91601562\n",
            "Step   1690: train CrossEntropyLoss |  0.15404174\n",
            "Step   1690: eval  CrossEntropyLoss |  0.19028568\n",
            "Step   1690: eval          Accuracy |  0.91796875\n",
            "Step   1700: train CrossEntropyLoss |  0.18456054\n",
            "Step   1700: eval  CrossEntropyLoss |  0.18400962\n",
            "Step   1700: eval          Accuracy |  0.92773438\n",
            "Step   1710: train CrossEntropyLoss |  0.21260281\n",
            "Step   1710: eval  CrossEntropyLoss |  0.19239249\n",
            "Step   1710: eval          Accuracy |  0.91992188\n",
            "Step   1720: train CrossEntropyLoss |  0.18075030\n",
            "Step   1720: eval  CrossEntropyLoss |  0.18861558\n",
            "Step   1720: eval          Accuracy |  0.92382812\n",
            "Step   1730: train CrossEntropyLoss |  0.16753232\n",
            "Step   1730: eval  CrossEntropyLoss |  0.17544586\n",
            "Step   1730: eval          Accuracy |  0.93164062\n",
            "Step   1740: train CrossEntropyLoss |  0.19086556\n",
            "Step   1740: eval  CrossEntropyLoss |  0.16891787\n",
            "Step   1740: eval          Accuracy |  0.92578125\n",
            "Step   1750: train CrossEntropyLoss |  0.15951942\n",
            "Step   1750: eval  CrossEntropyLoss |  0.17278497\n",
            "Step   1750: eval          Accuracy |  0.93945312\n",
            "Step   1760: train CrossEntropyLoss |  0.20661445\n",
            "Step   1760: eval  CrossEntropyLoss |  0.22150929\n",
            "Step   1760: eval          Accuracy |  0.91210938\n",
            "Step   1770: train CrossEntropyLoss |  0.15691374\n",
            "Step   1770: eval  CrossEntropyLoss |  0.16659350\n",
            "Step   1770: eval          Accuracy |  0.93945312\n",
            "Step   1780: train CrossEntropyLoss |  0.26888743\n",
            "Step   1780: eval  CrossEntropyLoss |  0.13570518\n",
            "Step   1780: eval          Accuracy |  0.95117188\n",
            "Step   1790: train CrossEntropyLoss |  0.20959997\n",
            "Step   1790: eval  CrossEntropyLoss |  0.16839763\n",
            "Step   1790: eval          Accuracy |  0.92968750\n",
            "Step   1800: train CrossEntropyLoss |  0.11364531\n",
            "Step   1800: eval  CrossEntropyLoss |  0.15987846\n",
            "Step   1800: eval          Accuracy |  0.93750000\n",
            "Step   1810: train CrossEntropyLoss |  0.21920319\n",
            "Step   1810: eval  CrossEntropyLoss |  0.20440798\n",
            "Step   1810: eval          Accuracy |  0.91601562\n",
            "Step   1820: train CrossEntropyLoss |  0.23886453\n",
            "Step   1820: eval  CrossEntropyLoss |  0.17891103\n",
            "Step   1820: eval          Accuracy |  0.92187500\n",
            "Step   1830: train CrossEntropyLoss |  0.15728000\n",
            "Step   1830: eval  CrossEntropyLoss |  0.14869183\n",
            "Step   1830: eval          Accuracy |  0.95312500\n",
            "Step   1840: train CrossEntropyLoss |  0.22522505\n",
            "Step   1840: eval  CrossEntropyLoss |  0.19677704\n",
            "Step   1840: eval          Accuracy |  0.91210938\n",
            "Step   1850: train CrossEntropyLoss |  0.26217300\n",
            "Step   1850: eval  CrossEntropyLoss |  0.21841378\n",
            "Step   1850: eval          Accuracy |  0.90234375\n",
            "Step   1860: train CrossEntropyLoss |  0.12624541\n",
            "Step   1860: eval  CrossEntropyLoss |  0.15149564\n",
            "Step   1860: eval          Accuracy |  0.94531250\n",
            "Step   1870: train CrossEntropyLoss |  0.11934008\n",
            "Step   1870: eval  CrossEntropyLoss |  0.16345214\n",
            "Step   1870: eval          Accuracy |  0.93750000\n",
            "Step   1880: train CrossEntropyLoss |  0.19457798\n",
            "Step   1880: eval  CrossEntropyLoss |  0.16739902\n",
            "Step   1880: eval          Accuracy |  0.93554688\n",
            "Step   1890: train CrossEntropyLoss |  0.12481742\n",
            "Step   1890: eval  CrossEntropyLoss |  0.20275719\n",
            "Step   1890: eval          Accuracy |  0.92968750\n",
            "Step   1900: train CrossEntropyLoss |  0.16436483\n",
            "Step   1900: eval  CrossEntropyLoss |  0.15776354\n",
            "Step   1900: eval          Accuracy |  0.93945312\n",
            "Step   1910: train CrossEntropyLoss |  0.12045548\n",
            "Step   1910: eval  CrossEntropyLoss |  0.17068043\n",
            "Step   1910: eval          Accuracy |  0.93164062\n",
            "Step   1920: train CrossEntropyLoss |  0.21745820\n",
            "Step   1920: eval  CrossEntropyLoss |  0.15180286\n",
            "Step   1920: eval          Accuracy |  0.93750000\n",
            "Step   1930: train CrossEntropyLoss |  0.14956552\n",
            "Step   1930: eval  CrossEntropyLoss |  0.20352168\n",
            "Step   1930: eval          Accuracy |  0.92382812\n",
            "Step   1940: train CrossEntropyLoss |  0.15012468\n",
            "Step   1940: eval  CrossEntropyLoss |  0.20962743\n",
            "Step   1940: eval          Accuracy |  0.91796875\n",
            "Step   1950: train CrossEntropyLoss |  0.15024774\n",
            "Step   1950: eval  CrossEntropyLoss |  0.15105900\n",
            "Step   1950: eval          Accuracy |  0.93554688\n",
            "Step   1960: train CrossEntropyLoss |  0.11686077\n",
            "Step   1960: eval  CrossEntropyLoss |  0.20264756\n",
            "Step   1960: eval          Accuracy |  0.93164062\n",
            "Step   1970: train CrossEntropyLoss |  0.17252661\n",
            "Step   1970: eval  CrossEntropyLoss |  0.17666374\n",
            "Step   1970: eval          Accuracy |  0.92773438\n",
            "Step   1980: train CrossEntropyLoss |  0.16719356\n",
            "Step   1980: eval  CrossEntropyLoss |  0.23211459\n",
            "Step   1980: eval          Accuracy |  0.91406250\n",
            "Step   1990: train CrossEntropyLoss |  0.12762158\n",
            "Step   1990: eval  CrossEntropyLoss |  0.13087103\n",
            "Step   1990: eval          Accuracy |  0.95507812\n",
            "Step   2000: train CrossEntropyLoss |  0.16812475\n",
            "Step   2000: eval  CrossEntropyLoss |  0.18825774\n",
            "Step   2000: eval          Accuracy |  0.92187500\n",
            "Step   2010: train CrossEntropyLoss |  0.06154377\n",
            "Step   2010: eval  CrossEntropyLoss |  0.15982250\n",
            "Step   2010: eval          Accuracy |  0.93359375\n",
            "Step   2020: train CrossEntropyLoss |  0.13689163\n",
            "Step   2020: eval  CrossEntropyLoss |  0.17161203\n",
            "Step   2020: eval          Accuracy |  0.93554688\n",
            "Step   2030: train CrossEntropyLoss |  0.21411894\n",
            "Step   2030: eval  CrossEntropyLoss |  0.15663508\n",
            "Step   2030: eval          Accuracy |  0.94335938\n",
            "Step   2040: train CrossEntropyLoss |  0.16399552\n",
            "Step   2040: eval  CrossEntropyLoss |  0.14750270\n",
            "Step   2040: eval          Accuracy |  0.93554688\n",
            "Step   2050: train CrossEntropyLoss |  0.16254330\n",
            "Step   2050: eval  CrossEntropyLoss |  0.20103950\n",
            "Step   2050: eval          Accuracy |  0.91210938\n",
            "Step   2060: train CrossEntropyLoss |  0.14013208\n",
            "Step   2060: eval  CrossEntropyLoss |  0.13991694\n",
            "Step   2060: eval          Accuracy |  0.93945312\n",
            "Step   2070: train CrossEntropyLoss |  0.15574260\n",
            "Step   2070: eval  CrossEntropyLoss |  0.11606547\n",
            "Step   2070: eval          Accuracy |  0.95898438\n",
            "Step   2080: train CrossEntropyLoss |  0.12462302\n",
            "Step   2080: eval  CrossEntropyLoss |  0.18506685\n",
            "Step   2080: eval          Accuracy |  0.92773438\n",
            "Step   2090: train CrossEntropyLoss |  0.18587495\n",
            "Step   2090: eval  CrossEntropyLoss |  0.15702677\n",
            "Step   2090: eval          Accuracy |  0.93554688\n",
            "Step   2100: train CrossEntropyLoss |  0.12110630\n",
            "Step   2100: eval  CrossEntropyLoss |  0.19170487\n",
            "Step   2100: eval          Accuracy |  0.92187500\n",
            "Step   2110: train CrossEntropyLoss |  0.10149161\n",
            "Step   2110: eval  CrossEntropyLoss |  0.21753082\n",
            "Step   2110: eval          Accuracy |  0.92187500\n",
            "Step   2120: train CrossEntropyLoss |  0.19955724\n",
            "Step   2120: eval  CrossEntropyLoss |  0.13996637\n",
            "Step   2120: eval          Accuracy |  0.94531250\n",
            "Step   2130: train CrossEntropyLoss |  0.13743906\n",
            "Step   2130: eval  CrossEntropyLoss |  0.21093798\n",
            "Step   2130: eval          Accuracy |  0.90429688\n",
            "Step   2140: train CrossEntropyLoss |  0.12402222\n",
            "Step   2140: eval  CrossEntropyLoss |  0.16995771\n",
            "Step   2140: eval          Accuracy |  0.93945312\n",
            "Step   2150: train CrossEntropyLoss |  0.13566785\n",
            "Step   2150: eval  CrossEntropyLoss |  0.15126075\n",
            "Step   2150: eval          Accuracy |  0.93750000\n",
            "Step   2160: train CrossEntropyLoss |  0.12307725\n",
            "Step   2160: eval  CrossEntropyLoss |  0.16614370\n",
            "Step   2160: eval          Accuracy |  0.93750000\n",
            "Step   2170: train CrossEntropyLoss |  0.17751627\n",
            "Step   2170: eval  CrossEntropyLoss |  0.18437472\n",
            "Step   2170: eval          Accuracy |  0.93359375\n",
            "Step   2180: train CrossEntropyLoss |  0.10124911\n",
            "Step   2180: eval  CrossEntropyLoss |  0.15966930\n",
            "Step   2180: eval          Accuracy |  0.94531250\n",
            "Step   2190: train CrossEntropyLoss |  0.15659146\n",
            "Step   2190: eval  CrossEntropyLoss |  0.12888963\n",
            "Step   2190: eval          Accuracy |  0.95117188\n",
            "Step   2200: train CrossEntropyLoss |  0.13542345\n",
            "Step   2200: eval  CrossEntropyLoss |  0.15843553\n",
            "Step   2200: eval          Accuracy |  0.93945312\n",
            "Step   2210: train CrossEntropyLoss |  0.15428974\n",
            "Step   2210: eval  CrossEntropyLoss |  0.15383807\n",
            "Step   2210: eval          Accuracy |  0.93164062\n",
            "Step   2220: train CrossEntropyLoss |  0.13114683\n",
            "Step   2220: eval  CrossEntropyLoss |  0.15629441\n",
            "Step   2220: eval          Accuracy |  0.94726562\n",
            "Step   2230: train CrossEntropyLoss |  0.10295536\n",
            "Step   2230: eval  CrossEntropyLoss |  0.13112106\n",
            "Step   2230: eval          Accuracy |  0.94140625\n",
            "Step   2240: train CrossEntropyLoss |  0.11702507\n",
            "Step   2240: eval  CrossEntropyLoss |  0.16465595\n",
            "Step   2240: eval          Accuracy |  0.93554688\n",
            "Step   2250: train CrossEntropyLoss |  0.16839865\n",
            "Step   2250: eval  CrossEntropyLoss |  0.13879152\n",
            "Step   2250: eval          Accuracy |  0.95312500\n",
            "Step   2260: train CrossEntropyLoss |  0.18966655\n",
            "Step   2260: eval  CrossEntropyLoss |  0.24191232\n",
            "Step   2260: eval          Accuracy |  0.91406250\n",
            "Step   2270: train CrossEntropyLoss |  0.12319224\n",
            "Step   2270: eval  CrossEntropyLoss |  0.17180219\n",
            "Step   2270: eval          Accuracy |  0.93945312\n",
            "Step   2280: train CrossEntropyLoss |  0.10509048\n",
            "Step   2280: eval  CrossEntropyLoss |  0.17182657\n",
            "Step   2280: eval          Accuracy |  0.92773438\n",
            "Step   2290: train CrossEntropyLoss |  0.14077264\n",
            "Step   2290: eval  CrossEntropyLoss |  0.17892508\n",
            "Step   2290: eval          Accuracy |  0.93554688\n",
            "Step   2300: train CrossEntropyLoss |  0.15038931\n",
            "Step   2300: eval  CrossEntropyLoss |  0.15529534\n",
            "Step   2300: eval          Accuracy |  0.92187500\n",
            "Step   2310: train CrossEntropyLoss |  0.08589602\n",
            "Step   2310: eval  CrossEntropyLoss |  0.14936294\n",
            "Step   2310: eval          Accuracy |  0.94335938\n",
            "Step   2320: train CrossEntropyLoss |  0.09233978\n",
            "Step   2320: eval  CrossEntropyLoss |  0.23876596\n",
            "Step   2320: eval          Accuracy |  0.90820312\n",
            "Step   2330: train CrossEntropyLoss |  0.06076632\n",
            "Step   2330: eval  CrossEntropyLoss |  0.15062288\n",
            "Step   2330: eval          Accuracy |  0.93945312\n",
            "Step   2340: train CrossEntropyLoss |  0.07864588\n",
            "Step   2340: eval  CrossEntropyLoss |  0.22818548\n",
            "Step   2340: eval          Accuracy |  0.92187500\n",
            "Step   2350: train CrossEntropyLoss |  0.07945143\n",
            "Step   2350: eval  CrossEntropyLoss |  0.18179082\n",
            "Step   2350: eval          Accuracy |  0.93164062\n",
            "Step   2360: train CrossEntropyLoss |  0.11059169\n",
            "Step   2360: eval  CrossEntropyLoss |  0.19249931\n",
            "Step   2360: eval          Accuracy |  0.94335938\n",
            "Step   2370: train CrossEntropyLoss |  0.13079727\n",
            "Step   2370: eval  CrossEntropyLoss |  0.18519440\n",
            "Step   2370: eval          Accuracy |  0.92968750\n",
            "Step   2380: train CrossEntropyLoss |  0.07392184\n",
            "Step   2380: eval  CrossEntropyLoss |  0.14014372\n",
            "Step   2380: eval          Accuracy |  0.94921875\n",
            "Step   2390: train CrossEntropyLoss |  0.13699415\n",
            "Step   2390: eval  CrossEntropyLoss |  0.18296580\n",
            "Step   2390: eval          Accuracy |  0.93359375\n",
            "Step   2400: train CrossEntropyLoss |  0.12938838\n",
            "Step   2400: eval  CrossEntropyLoss |  0.14400545\n",
            "Step   2400: eval          Accuracy |  0.94140625\n",
            "Step   2410: train CrossEntropyLoss |  0.06394839\n",
            "Step   2410: eval  CrossEntropyLoss |  0.18430449\n",
            "Step   2410: eval          Accuracy |  0.93750000\n",
            "Step   2420: train CrossEntropyLoss |  0.07113682\n",
            "Step   2420: eval  CrossEntropyLoss |  0.20130396\n",
            "Step   2420: eval          Accuracy |  0.92187500\n",
            "Step   2430: train CrossEntropyLoss |  0.07886761\n",
            "Step   2430: eval  CrossEntropyLoss |  0.19389426\n",
            "Step   2430: eval          Accuracy |  0.92968750\n",
            "Step   2440: train CrossEntropyLoss |  0.15707369\n",
            "Step   2440: eval  CrossEntropyLoss |  0.17995535\n",
            "Step   2440: eval          Accuracy |  0.93164062\n",
            "Step   2450: train CrossEntropyLoss |  0.09751176\n",
            "Step   2450: eval  CrossEntropyLoss |  0.14930723\n",
            "Step   2450: eval          Accuracy |  0.94335938\n",
            "Step   2460: train CrossEntropyLoss |  0.07176708\n",
            "Step   2460: eval  CrossEntropyLoss |  0.12518781\n",
            "Step   2460: eval          Accuracy |  0.95117188\n",
            "Step   2470: train CrossEntropyLoss |  0.18265457\n",
            "Step   2470: eval  CrossEntropyLoss |  0.15764926\n",
            "Step   2470: eval          Accuracy |  0.94726562\n",
            "Step   2480: train CrossEntropyLoss |  0.10156641\n",
            "Step   2480: eval  CrossEntropyLoss |  0.15935983\n",
            "Step   2480: eval          Accuracy |  0.93750000\n",
            "Step   2490: train CrossEntropyLoss |  0.10361271\n",
            "Step   2490: eval  CrossEntropyLoss |  0.16642257\n",
            "Step   2490: eval          Accuracy |  0.94335938\n",
            "Step   2500: train CrossEntropyLoss |  0.17537498\n",
            "Step   2500: eval  CrossEntropyLoss |  0.25652991\n",
            "Step   2500: eval          Accuracy |  0.91601562\n",
            "Step   2510: train CrossEntropyLoss |  0.09193989\n",
            "Step   2510: eval  CrossEntropyLoss |  0.17433091\n",
            "Step   2510: eval          Accuracy |  0.92968750\n",
            "Step   2520: train CrossEntropyLoss |  0.11759768\n",
            "Step   2520: eval  CrossEntropyLoss |  0.14823541\n",
            "Step   2520: eval          Accuracy |  0.95312500\n",
            "Step   2530: train CrossEntropyLoss |  0.16320534\n",
            "Step   2530: eval  CrossEntropyLoss |  0.21019904\n",
            "Step   2530: eval          Accuracy |  0.94140625\n",
            "Step   2540: train CrossEntropyLoss |  0.10438774\n",
            "Step   2540: eval  CrossEntropyLoss |  0.16313490\n",
            "Step   2540: eval          Accuracy |  0.93359375\n",
            "Step   2550: train CrossEntropyLoss |  0.08613521\n",
            "Step   2550: eval  CrossEntropyLoss |  0.15362212\n",
            "Step   2550: eval          Accuracy |  0.92968750\n",
            "Step   2560: train CrossEntropyLoss |  0.09835667\n",
            "Step   2560: eval  CrossEntropyLoss |  0.19756927\n",
            "Step   2560: eval          Accuracy |  0.93359375\n",
            "Step   2570: train CrossEntropyLoss |  0.06557540\n",
            "Step   2570: eval  CrossEntropyLoss |  0.09348402\n",
            "Step   2570: eval          Accuracy |  0.95898438\n",
            "Step   2580: train CrossEntropyLoss |  0.11429610\n",
            "Step   2580: eval  CrossEntropyLoss |  0.16215514\n",
            "Step   2580: eval          Accuracy |  0.93750000\n",
            "Step   2590: train CrossEntropyLoss |  0.16344552\n",
            "Step   2590: eval  CrossEntropyLoss |  0.15723527\n",
            "Step   2590: eval          Accuracy |  0.94335938\n",
            "Step   2600: train CrossEntropyLoss |  0.06557466\n",
            "Step   2600: eval  CrossEntropyLoss |  0.22139950\n",
            "Step   2600: eval          Accuracy |  0.92968750\n",
            "Step   2610: train CrossEntropyLoss |  0.11090008\n",
            "Step   2610: eval  CrossEntropyLoss |  0.19026668\n",
            "Step   2610: eval          Accuracy |  0.92968750\n",
            "Step   2620: train CrossEntropyLoss |  0.14964883\n",
            "Step   2620: eval  CrossEntropyLoss |  0.14995393\n",
            "Step   2620: eval          Accuracy |  0.94140625\n",
            "Step   2630: train CrossEntropyLoss |  0.13995801\n",
            "Step   2630: eval  CrossEntropyLoss |  0.16456070\n",
            "Step   2630: eval          Accuracy |  0.93750000\n",
            "Step   2640: train CrossEntropyLoss |  0.06177316\n",
            "Step   2640: eval  CrossEntropyLoss |  0.13151780\n",
            "Step   2640: eval          Accuracy |  0.94140625\n",
            "Step   2650: train CrossEntropyLoss |  0.05330334\n",
            "Step   2650: eval  CrossEntropyLoss |  0.16250963\n",
            "Step   2650: eval          Accuracy |  0.94531250\n",
            "Step   2660: train CrossEntropyLoss |  0.16265218\n",
            "Step   2660: eval  CrossEntropyLoss |  0.15024396\n",
            "Step   2660: eval          Accuracy |  0.94726562\n",
            "Step   2670: train CrossEntropyLoss |  0.09200600\n",
            "Step   2670: eval  CrossEntropyLoss |  0.14017069\n",
            "Step   2670: eval          Accuracy |  0.95312500\n",
            "Step   2680: train CrossEntropyLoss |  0.07213955\n",
            "Step   2680: eval  CrossEntropyLoss |  0.20314856\n",
            "Step   2680: eval          Accuracy |  0.91601562\n",
            "Step   2690: train CrossEntropyLoss |  0.04341786\n",
            "Step   2690: eval  CrossEntropyLoss |  0.15240960\n",
            "Step   2690: eval          Accuracy |  0.94531250\n",
            "Step   2700: train CrossEntropyLoss |  0.13058043\n",
            "Step   2700: eval  CrossEntropyLoss |  0.19382884\n",
            "Step   2700: eval          Accuracy |  0.93359375\n",
            "Step   2710: train CrossEntropyLoss |  0.04749103\n",
            "Step   2710: eval  CrossEntropyLoss |  0.22150992\n",
            "Step   2710: eval          Accuracy |  0.90429688\n",
            "Step   2720: train CrossEntropyLoss |  0.13950400\n",
            "Step   2720: eval  CrossEntropyLoss |  0.16686795\n",
            "Step   2720: eval          Accuracy |  0.94140625\n",
            "Step   2730: train CrossEntropyLoss |  0.10190766\n",
            "Step   2730: eval  CrossEntropyLoss |  0.12198830\n",
            "Step   2730: eval          Accuracy |  0.95312500\n",
            "Step   2740: train CrossEntropyLoss |  0.07474186\n",
            "Step   2740: eval  CrossEntropyLoss |  0.15473074\n",
            "Step   2740: eval          Accuracy |  0.94335938\n",
            "Step   2750: train CrossEntropyLoss |  0.12814619\n",
            "Step   2750: eval  CrossEntropyLoss |  0.18847044\n",
            "Step   2750: eval          Accuracy |  0.93164062\n",
            "Step   2760: train CrossEntropyLoss |  0.09702923\n",
            "Step   2760: eval  CrossEntropyLoss |  0.17044456\n",
            "Step   2760: eval          Accuracy |  0.93750000\n",
            "Step   2770: train CrossEntropyLoss |  0.07339262\n",
            "Step   2770: eval  CrossEntropyLoss |  0.19361606\n",
            "Step   2770: eval          Accuracy |  0.93359375\n",
            "Step   2780: train CrossEntropyLoss |  0.14532146\n",
            "Step   2780: eval  CrossEntropyLoss |  0.24773325\n",
            "Step   2780: eval          Accuracy |  0.91210938\n",
            "Step   2790: train CrossEntropyLoss |  0.18358825\n",
            "Step   2790: eval  CrossEntropyLoss |  0.23763572\n",
            "Step   2790: eval          Accuracy |  0.93164062\n",
            "Step   2800: train CrossEntropyLoss |  0.09922147\n",
            "Step   2800: eval  CrossEntropyLoss |  0.14362968\n",
            "Step   2800: eval          Accuracy |  0.94335938\n",
            "Step   2810: train CrossEntropyLoss |  0.04635837\n",
            "Step   2810: eval  CrossEntropyLoss |  0.16102407\n",
            "Step   2810: eval          Accuracy |  0.94140625\n",
            "Step   2820: train CrossEntropyLoss |  0.11719353\n",
            "Step   2820: eval  CrossEntropyLoss |  0.20818375\n",
            "Step   2820: eval          Accuracy |  0.91210938\n",
            "Step   2830: train CrossEntropyLoss |  0.13146274\n",
            "Step   2830: eval  CrossEntropyLoss |  0.13756257\n",
            "Step   2830: eval          Accuracy |  0.94335938\n",
            "Step   2840: train CrossEntropyLoss |  0.14154162\n",
            "Step   2840: eval  CrossEntropyLoss |  0.17399071\n",
            "Step   2840: eval          Accuracy |  0.93359375\n",
            "Step   2850: train CrossEntropyLoss |  0.07904037\n",
            "Step   2850: eval  CrossEntropyLoss |  0.16303892\n",
            "Step   2850: eval          Accuracy |  0.93164062\n",
            "Step   2860: train CrossEntropyLoss |  0.13940422\n",
            "Step   2860: eval  CrossEntropyLoss |  0.22785677\n",
            "Step   2860: eval          Accuracy |  0.90625000\n",
            "Step   2870: train CrossEntropyLoss |  0.07518335\n",
            "Step   2870: eval  CrossEntropyLoss |  0.16218166\n",
            "Step   2870: eval          Accuracy |  0.93359375\n",
            "Step   2880: train CrossEntropyLoss |  0.13402557\n",
            "Step   2880: eval  CrossEntropyLoss |  0.16947553\n",
            "Step   2880: eval          Accuracy |  0.93945312\n",
            "Step   2890: train CrossEntropyLoss |  0.12210546\n",
            "Step   2890: eval  CrossEntropyLoss |  0.15558114\n",
            "Step   2890: eval          Accuracy |  0.93945312\n",
            "Step   2900: train CrossEntropyLoss |  0.06309272\n",
            "Step   2900: eval  CrossEntropyLoss |  0.15619365\n",
            "Step   2900: eval          Accuracy |  0.94335938\n",
            "Step   2910: train CrossEntropyLoss |  0.06593209\n",
            "Step   2910: eval  CrossEntropyLoss |  0.20254951\n",
            "Step   2910: eval          Accuracy |  0.92578125\n",
            "Step   2920: train CrossEntropyLoss |  0.04836402\n",
            "Step   2920: eval  CrossEntropyLoss |  0.21034345\n",
            "Step   2920: eval          Accuracy |  0.93164062\n",
            "Step   2930: train CrossEntropyLoss |  0.08060483\n",
            "Step   2930: eval  CrossEntropyLoss |  0.11450613\n",
            "Step   2930: eval          Accuracy |  0.96093750\n",
            "Step   2940: train CrossEntropyLoss |  0.09813050\n",
            "Step   2940: eval  CrossEntropyLoss |  0.14931751\n",
            "Step   2940: eval          Accuracy |  0.94335938\n",
            "Step   2950: train CrossEntropyLoss |  0.16304371\n",
            "Step   2950: eval  CrossEntropyLoss |  0.13635469\n",
            "Step   2950: eval          Accuracy |  0.95703125\n",
            "Step   2960: train CrossEntropyLoss |  0.05138929\n",
            "Step   2960: eval  CrossEntropyLoss |  0.12432139\n",
            "Step   2960: eval          Accuracy |  0.94335938\n",
            "Step   2970: train CrossEntropyLoss |  0.09597327\n",
            "Step   2970: eval  CrossEntropyLoss |  0.22779660\n",
            "Step   2970: eval          Accuracy |  0.91406250\n",
            "Step   2980: train CrossEntropyLoss |  0.12719163\n",
            "Step   2980: eval  CrossEntropyLoss |  0.17422439\n",
            "Step   2980: eval          Accuracy |  0.93750000\n",
            "Step   2990: train CrossEntropyLoss |  0.14537057\n",
            "Step   2990: eval  CrossEntropyLoss |  0.18010652\n",
            "Step   2990: eval          Accuracy |  0.93164062\n",
            "Step   3000: train CrossEntropyLoss |  0.11925979\n",
            "Step   3000: eval  CrossEntropyLoss |  0.16392853\n",
            "Step   3000: eval          Accuracy |  0.94335938\n",
            "Step   3010: train CrossEntropyLoss |  0.10464676\n",
            "Step   3010: eval  CrossEntropyLoss |  0.11471330\n",
            "Step   3010: eval          Accuracy |  0.94726562\n",
            "Step   3020: train CrossEntropyLoss |  0.12236786\n",
            "Step   3020: eval  CrossEntropyLoss |  0.16609380\n",
            "Step   3020: eval          Accuracy |  0.93554688\n",
            "Step   3030: train CrossEntropyLoss |  0.07894654\n",
            "Step   3030: eval  CrossEntropyLoss |  0.18635109\n",
            "Step   3030: eval          Accuracy |  0.92968750\n",
            "Step   3040: train CrossEntropyLoss |  0.06973603\n",
            "Step   3040: eval  CrossEntropyLoss |  0.17856888\n",
            "Step   3040: eval          Accuracy |  0.93554688\n",
            "Step   3050: train CrossEntropyLoss |  0.10578997\n",
            "Step   3050: eval  CrossEntropyLoss |  0.17509791\n",
            "Step   3050: eval          Accuracy |  0.93750000\n",
            "Step   3060: train CrossEntropyLoss |  0.08924026\n",
            "Step   3060: eval  CrossEntropyLoss |  0.15629888\n",
            "Step   3060: eval          Accuracy |  0.94140625\n",
            "Step   3070: train CrossEntropyLoss |  0.14819111\n",
            "Step   3070: eval  CrossEntropyLoss |  0.15239903\n",
            "Step   3070: eval          Accuracy |  0.94335938\n",
            "Step   3080: train CrossEntropyLoss |  0.10988513\n",
            "Step   3080: eval  CrossEntropyLoss |  0.15257655\n",
            "Step   3080: eval          Accuracy |  0.94140625\n",
            "Step   3090: train CrossEntropyLoss |  0.08070763\n",
            "Step   3090: eval  CrossEntropyLoss |  0.16282770\n",
            "Step   3090: eval          Accuracy |  0.93359375\n",
            "Step   3100: train CrossEntropyLoss |  0.16739129\n",
            "Step   3100: eval  CrossEntropyLoss |  0.16355341\n",
            "Step   3100: eval          Accuracy |  0.93359375\n",
            "Step   3110: train CrossEntropyLoss |  0.11632340\n",
            "Step   3110: eval  CrossEntropyLoss |  0.19443140\n",
            "Step   3110: eval          Accuracy |  0.92382812\n",
            "Step   3120: train CrossEntropyLoss |  0.09972315\n",
            "Step   3120: eval  CrossEntropyLoss |  0.15094651\n",
            "Step   3120: eval          Accuracy |  0.94335938\n",
            "Step   3130: train CrossEntropyLoss |  0.05803395\n",
            "Step   3130: eval  CrossEntropyLoss |  0.16642915\n",
            "Step   3130: eval          Accuracy |  0.94921875\n",
            "Step   3140: train CrossEntropyLoss |  0.09187256\n",
            "Step   3140: eval  CrossEntropyLoss |  0.15416059\n",
            "Step   3140: eval          Accuracy |  0.93554688\n",
            "Step   3150: train CrossEntropyLoss |  0.13952902\n",
            "Step   3150: eval  CrossEntropyLoss |  0.16004044\n",
            "Step   3150: eval          Accuracy |  0.93945312\n",
            "Step   3160: train CrossEntropyLoss |  0.08673968\n",
            "Step   3160: eval  CrossEntropyLoss |  0.16467668\n",
            "Step   3160: eval          Accuracy |  0.94921875\n",
            "Step   3170: train CrossEntropyLoss |  0.05944343\n",
            "Step   3170: eval  CrossEntropyLoss |  0.14496304\n",
            "Step   3170: eval          Accuracy |  0.94531250\n",
            "Step   3180: train CrossEntropyLoss |  0.14003964\n",
            "Step   3180: eval  CrossEntropyLoss |  0.16412506\n",
            "Step   3180: eval          Accuracy |  0.93554688\n",
            "Step   3190: train CrossEntropyLoss |  0.12684618\n",
            "Step   3190: eval  CrossEntropyLoss |  0.14262417\n",
            "Step   3190: eval          Accuracy |  0.95117188\n",
            "Step   3200: train CrossEntropyLoss |  0.07946159\n",
            "Step   3200: eval  CrossEntropyLoss |  0.15568734\n",
            "Step   3200: eval          Accuracy |  0.93945312\n",
            "Step   3210: train CrossEntropyLoss |  0.14254652\n",
            "Step   3210: eval  CrossEntropyLoss |  0.19517319\n",
            "Step   3210: eval          Accuracy |  0.91796875\n",
            "Step   3220: train CrossEntropyLoss |  0.06187018\n",
            "Step   3220: eval  CrossEntropyLoss |  0.19439242\n",
            "Step   3220: eval          Accuracy |  0.92773438\n",
            "Step   3230: train CrossEntropyLoss |  0.05314339\n",
            "Step   3230: eval  CrossEntropyLoss |  0.15677046\n",
            "Step   3230: eval          Accuracy |  0.93554688\n",
            "Step   3240: train CrossEntropyLoss |  0.12230127\n",
            "Step   3240: eval  CrossEntropyLoss |  0.17585682\n",
            "Step   3240: eval          Accuracy |  0.94335938\n",
            "Step   3250: train CrossEntropyLoss |  0.09365314\n",
            "Step   3250: eval  CrossEntropyLoss |  0.16933675\n",
            "Step   3250: eval          Accuracy |  0.93750000\n",
            "Step   3260: train CrossEntropyLoss |  0.15884352\n",
            "Step   3260: eval  CrossEntropyLoss |  0.15290547\n",
            "Step   3260: eval          Accuracy |  0.93945312\n",
            "Step   3270: train CrossEntropyLoss |  0.16214029\n",
            "Step   3270: eval  CrossEntropyLoss |  0.15212192\n",
            "Step   3270: eval          Accuracy |  0.93750000\n",
            "Step   3280: train CrossEntropyLoss |  0.07008550\n",
            "Step   3280: eval  CrossEntropyLoss |  0.14266043\n",
            "Step   3280: eval          Accuracy |  0.94140625\n",
            "Step   3290: train CrossEntropyLoss |  0.07002245\n",
            "Step   3290: eval  CrossEntropyLoss |  0.16053568\n",
            "Step   3290: eval          Accuracy |  0.93750000\n",
            "Step   3300: train CrossEntropyLoss |  0.08048590\n",
            "Step   3300: eval  CrossEntropyLoss |  0.18778433\n",
            "Step   3300: eval          Accuracy |  0.93359375\n",
            "Step   3310: train CrossEntropyLoss |  0.05787329\n",
            "Step   3310: eval  CrossEntropyLoss |  0.12643524\n",
            "Step   3310: eval          Accuracy |  0.95312500\n",
            "Step   3320: train CrossEntropyLoss |  0.04197663\n",
            "Step   3320: eval  CrossEntropyLoss |  0.17525496\n",
            "Step   3320: eval          Accuracy |  0.93750000\n",
            "Step   3330: train CrossEntropyLoss |  0.09235079\n",
            "Step   3330: eval  CrossEntropyLoss |  0.19469971\n",
            "Step   3330: eval          Accuracy |  0.93750000\n",
            "Step   3340: train CrossEntropyLoss |  0.10202193\n",
            "Step   3340: eval  CrossEntropyLoss |  0.15970514\n",
            "Step   3340: eval          Accuracy |  0.93945312\n",
            "Step   3350: train CrossEntropyLoss |  0.15774415\n",
            "Step   3350: eval  CrossEntropyLoss |  0.16230465\n",
            "Step   3350: eval          Accuracy |  0.93945312\n",
            "Step   3360: train CrossEntropyLoss |  0.06753591\n",
            "Step   3360: eval  CrossEntropyLoss |  0.16685584\n",
            "Step   3360: eval          Accuracy |  0.93945312\n",
            "Step   3370: train CrossEntropyLoss |  0.09567817\n",
            "Step   3370: eval  CrossEntropyLoss |  0.12310825\n",
            "Step   3370: eval          Accuracy |  0.95117188\n",
            "Step   3380: train CrossEntropyLoss |  0.11777756\n",
            "Step   3380: eval  CrossEntropyLoss |  0.15209505\n",
            "Step   3380: eval          Accuracy |  0.95117188\n",
            "Step   3390: train CrossEntropyLoss |  0.10348554\n",
            "Step   3390: eval  CrossEntropyLoss |  0.17794131\n",
            "Step   3390: eval          Accuracy |  0.94921875\n",
            "Step   3400: train CrossEntropyLoss |  0.08640092\n",
            "Step   3400: eval  CrossEntropyLoss |  0.14190926\n",
            "Step   3400: eval          Accuracy |  0.95703125\n",
            "Step   3410: train CrossEntropyLoss |  0.05181599\n",
            "Step   3410: eval  CrossEntropyLoss |  0.18665718\n",
            "Step   3410: eval          Accuracy |  0.92773438\n",
            "Step   3420: train CrossEntropyLoss |  0.06753657\n",
            "Step   3420: eval  CrossEntropyLoss |  0.15617411\n",
            "Step   3420: eval          Accuracy |  0.94726562\n",
            "Step   3430: train CrossEntropyLoss |  0.12760310\n",
            "Step   3430: eval  CrossEntropyLoss |  0.18201711\n",
            "Step   3430: eval          Accuracy |  0.93945312\n",
            "Step   3440: train CrossEntropyLoss |  0.05016727\n",
            "Step   3440: eval  CrossEntropyLoss |  0.13303627\n",
            "Step   3440: eval          Accuracy |  0.94140625\n",
            "Step   3450: train CrossEntropyLoss |  0.07644454\n",
            "Step   3450: eval  CrossEntropyLoss |  0.14695731\n",
            "Step   3450: eval          Accuracy |  0.94335938\n",
            "Step   3460: train CrossEntropyLoss |  0.04387643\n",
            "Step   3460: eval  CrossEntropyLoss |  0.14868082\n",
            "Step   3460: eval          Accuracy |  0.94140625\n",
            "Step   3470: train CrossEntropyLoss |  0.02727379\n",
            "Step   3470: eval  CrossEntropyLoss |  0.15141114\n",
            "Step   3470: eval          Accuracy |  0.93750000\n",
            "Step   3480: train CrossEntropyLoss |  0.03357382\n",
            "Step   3480: eval  CrossEntropyLoss |  0.12816848\n",
            "Step   3480: eval          Accuracy |  0.96289062\n",
            "Step   3490: train CrossEntropyLoss |  0.07694124\n",
            "Step   3490: eval  CrossEntropyLoss |  0.18363086\n",
            "Step   3490: eval          Accuracy |  0.93945312\n",
            "Step   3500: train CrossEntropyLoss |  0.03638515\n",
            "Step   3500: eval  CrossEntropyLoss |  0.15213011\n",
            "Step   3500: eval          Accuracy |  0.94531250\n",
            "Step   3510: train CrossEntropyLoss |  0.03065298\n",
            "Step   3510: eval  CrossEntropyLoss |  0.24516885\n",
            "Step   3510: eval          Accuracy |  0.91992188\n",
            "Step   3520: train CrossEntropyLoss |  0.07813398\n",
            "Step   3520: eval  CrossEntropyLoss |  0.22551951\n",
            "Step   3520: eval          Accuracy |  0.92773438\n",
            "Step   3530: train CrossEntropyLoss |  0.13183740\n",
            "Step   3530: eval  CrossEntropyLoss |  0.13492799\n",
            "Step   3530: eval          Accuracy |  0.95507812\n",
            "Step   3540: train CrossEntropyLoss |  0.07787748\n",
            "Step   3540: eval  CrossEntropyLoss |  0.21543865\n",
            "Step   3540: eval          Accuracy |  0.93554688\n",
            "Step   3550: train CrossEntropyLoss |  0.05137516\n",
            "Step   3550: eval  CrossEntropyLoss |  0.18501388\n",
            "Step   3550: eval          Accuracy |  0.92968750\n",
            "Step   3560: train CrossEntropyLoss |  0.07765966\n",
            "Step   3560: eval  CrossEntropyLoss |  0.12304195\n",
            "Step   3560: eval          Accuracy |  0.95312500\n",
            "Step   3570: train CrossEntropyLoss |  0.03829737\n",
            "Step   3570: eval  CrossEntropyLoss |  0.14502514\n",
            "Step   3570: eval          Accuracy |  0.95703125\n",
            "Step   3580: train CrossEntropyLoss |  0.08095016\n",
            "Step   3580: eval  CrossEntropyLoss |  0.17350099\n",
            "Step   3580: eval          Accuracy |  0.93554688\n",
            "Step   3590: train CrossEntropyLoss |  0.13303217\n",
            "Step   3590: eval  CrossEntropyLoss |  0.15566491\n",
            "Step   3590: eval          Accuracy |  0.94140625\n",
            "Step   3600: train CrossEntropyLoss |  0.08426201\n",
            "Step   3600: eval  CrossEntropyLoss |  0.12684014\n",
            "Step   3600: eval          Accuracy |  0.95312500\n",
            "Step   3610: train CrossEntropyLoss |  0.07734104\n",
            "Step   3610: eval  CrossEntropyLoss |  0.14355354\n",
            "Step   3610: eval          Accuracy |  0.93554688\n",
            "Step   3620: train CrossEntropyLoss |  0.06758936\n",
            "Step   3620: eval  CrossEntropyLoss |  0.12835797\n",
            "Step   3620: eval          Accuracy |  0.94140625\n",
            "Step   3630: train CrossEntropyLoss |  0.03698200\n",
            "Step   3630: eval  CrossEntropyLoss |  0.19353924\n",
            "Step   3630: eval          Accuracy |  0.92382812\n",
            "Step   3640: train CrossEntropyLoss |  0.03899787\n",
            "Step   3640: eval  CrossEntropyLoss |  0.12601991\n",
            "Step   3640: eval          Accuracy |  0.94335938\n",
            "Step   3650: train CrossEntropyLoss |  0.05989848\n",
            "Step   3650: eval  CrossEntropyLoss |  0.20110277\n",
            "Step   3650: eval          Accuracy |  0.92773438\n",
            "Step   3660: train CrossEntropyLoss |  0.08773485\n",
            "Step   3660: eval  CrossEntropyLoss |  0.18104629\n",
            "Step   3660: eval          Accuracy |  0.93359375\n",
            "Step   3670: train CrossEntropyLoss |  0.04082579\n",
            "Step   3670: eval  CrossEntropyLoss |  0.13874603\n",
            "Step   3670: eval          Accuracy |  0.94921875\n",
            "Step   3680: train CrossEntropyLoss |  0.07659985\n",
            "Step   3680: eval  CrossEntropyLoss |  0.13557575\n",
            "Step   3680: eval          Accuracy |  0.95312500\n",
            "Step   3690: train CrossEntropyLoss |  0.04200866\n",
            "Step   3690: eval  CrossEntropyLoss |  0.16390821\n",
            "Step   3690: eval          Accuracy |  0.94335938\n",
            "Step   3700: train CrossEntropyLoss |  0.04497616\n",
            "Step   3700: eval  CrossEntropyLoss |  0.11413624\n",
            "Step   3700: eval          Accuracy |  0.95117188\n",
            "Step   3710: train CrossEntropyLoss |  0.04072256\n",
            "Step   3710: eval  CrossEntropyLoss |  0.15083296\n",
            "Step   3710: eval          Accuracy |  0.94335938\n",
            "Step   3720: train CrossEntropyLoss |  0.04929419\n",
            "Step   3720: eval  CrossEntropyLoss |  0.14745288\n",
            "Step   3720: eval          Accuracy |  0.95703125\n",
            "Step   3730: train CrossEntropyLoss |  0.03832196\n",
            "Step   3730: eval  CrossEntropyLoss |  0.20263592\n",
            "Step   3730: eval          Accuracy |  0.93164062\n",
            "Step   3740: train CrossEntropyLoss |  0.04588179\n",
            "Step   3740: eval  CrossEntropyLoss |  0.12845297\n",
            "Step   3740: eval          Accuracy |  0.94921875\n",
            "Step   3750: train CrossEntropyLoss |  0.03181646\n",
            "Step   3750: eval  CrossEntropyLoss |  0.10110002\n",
            "Step   3750: eval          Accuracy |  0.95898438\n",
            "Step   3760: train CrossEntropyLoss |  0.07194060\n",
            "Step   3760: eval  CrossEntropyLoss |  0.22417731\n",
            "Step   3760: eval          Accuracy |  0.93164062\n",
            "Step   3770: train CrossEntropyLoss |  0.06104571\n",
            "Step   3770: eval  CrossEntropyLoss |  0.14757044\n",
            "Step   3770: eval          Accuracy |  0.94140625\n",
            "Step   3780: train CrossEntropyLoss |  0.03312035\n",
            "Step   3780: eval  CrossEntropyLoss |  0.17520041\n",
            "Step   3780: eval          Accuracy |  0.94531250\n",
            "Step   3790: train CrossEntropyLoss |  0.04801543\n",
            "Step   3790: eval  CrossEntropyLoss |  0.22337612\n",
            "Step   3790: eval          Accuracy |  0.93359375\n",
            "Step   3800: train CrossEntropyLoss |  0.04520196\n",
            "Step   3800: eval  CrossEntropyLoss |  0.13550636\n",
            "Step   3800: eval          Accuracy |  0.95312500\n",
            "Step   3810: train CrossEntropyLoss |  0.06479584\n",
            "Step   3810: eval  CrossEntropyLoss |  0.19782644\n",
            "Step   3810: eval          Accuracy |  0.93945312\n",
            "Step   3820: train CrossEntropyLoss |  0.04099080\n",
            "Step   3820: eval  CrossEntropyLoss |  0.15676350\n",
            "Step   3820: eval          Accuracy |  0.94335938\n",
            "Step   3830: train CrossEntropyLoss |  0.07373354\n",
            "Step   3830: eval  CrossEntropyLoss |  0.27690369\n",
            "Step   3830: eval          Accuracy |  0.92968750\n",
            "Step   3840: train CrossEntropyLoss |  0.07869327\n",
            "Step   3840: eval  CrossEntropyLoss |  0.12780124\n",
            "Step   3840: eval          Accuracy |  0.95117188\n",
            "Step   3850: train CrossEntropyLoss |  0.04817060\n",
            "Step   3850: eval  CrossEntropyLoss |  0.16181974\n",
            "Step   3850: eval          Accuracy |  0.94140625\n",
            "Step   3860: train CrossEntropyLoss |  0.05207441\n",
            "Step   3860: eval  CrossEntropyLoss |  0.22189487\n",
            "Step   3860: eval          Accuracy |  0.92968750\n",
            "Step   3870: train CrossEntropyLoss |  0.06169106\n",
            "Step   3870: eval  CrossEntropyLoss |  0.20760638\n",
            "Step   3870: eval          Accuracy |  0.93164062\n",
            "Step   3880: train CrossEntropyLoss |  0.06983552\n",
            "Step   3880: eval  CrossEntropyLoss |  0.14140831\n",
            "Step   3880: eval          Accuracy |  0.94921875\n",
            "Step   3890: train CrossEntropyLoss |  0.02211270\n",
            "Step   3890: eval  CrossEntropyLoss |  0.18192683\n",
            "Step   3890: eval          Accuracy |  0.94531250\n",
            "Step   3900: train CrossEntropyLoss |  0.04543307\n",
            "Step   3900: eval  CrossEntropyLoss |  0.15635050\n",
            "Step   3900: eval          Accuracy |  0.95507812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VwsalQF2lBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "089a2a00-647e-4945-88f4-b506d18d7b64"
      },
      "source": [
        "# Create a generator object\n",
        "tmp_train_generator = train_generator(16)\n",
        "\n",
        "# get one batch\n",
        "tmp_batch = next(tmp_train_generator)\n",
        "\n",
        "# Position 0 has the model inputs (tweets as tensors)\n",
        "# position 1 has the targets (the actual labels)\n",
        "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
        "\n",
        "print(f\"The batch is a tuple of length {len(tmp_batch)} because position 0 contains the tweets, and position 1 contains the targets.\") \n",
        "print(f\"The shape of the tweet tensors is {tmp_inputs.shape} (num of examples, length of tweet tensors)\")\n",
        "print(f\"The shape of the labels is {tmp_targets.shape}, which is the batch size.\")\n",
        "print(f\"The shape of the example_weights is {tmp_example_weights.shape}, which is the same as inputs/targets size.\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The batch is a tuple of length 3 because position 0 contains the tweets, and position 1 contains the targets.\n",
            "The shape of the tweet tensors is (16, 575) (num of examples, length of tweet tensors)\n",
            "The shape of the labels is (16,), which is the batch size.\n",
            "The shape of the example_weights is (16,), which is the same as inputs/targets size.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PThc9yEdskR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "4e0f8a4f-b449-4d04-ee53-3e9b2981bf61"
      },
      "source": [
        "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
        "print(f\"The prediction shape is {tmp_pred.shape}, num of tensor_tweets as rows\")\n",
        "print(\"Column 0 is the probability of a negative sentiment (class 0)\")\n",
        "print(\"Column 1 is the probability of a positive sentiment (class 1)\")\n",
        "print()\n",
        "print(\"View the prediction array\")\n",
        "tmp_pred"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The prediction shape is (16, 2), num of tensor_tweets as rows\n",
            "Column 0 is the probability of a negative sentiment (class 0)\n",
            "Column 1 is the probability of a positive sentiment (class 1)\n",
            "\n",
            "View the prediction array\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[-7.71493912e-02, -2.60033798e+00],\n",
              "             [-6.19888306e-06, -1.20189018e+01],\n",
              "             [-1.21895485e+01, -5.24520874e-06],\n",
              "             [-1.27900543e+01, -2.86102295e-06],\n",
              "             [-2.86102295e-06, -1.28113918e+01],\n",
              "             [-1.22818527e+01, -4.76837158e-06],\n",
              "             [-2.79283428e+00, -6.32033348e-02],\n",
              "             [-8.10623169e-06, -1.17256804e+01],\n",
              "             [-7.84419394e+00, -3.91960144e-04],\n",
              "             [-1.04904175e-05, -1.14506378e+01],\n",
              "             [-2.29597092e-04, -8.37881184e+00],\n",
              "             [-1.90734863e-06, -1.31815567e+01],\n",
              "             [-2.50172186e+00, -8.54965448e-02],\n",
              "             [-1.24684887e+01, -3.81469727e-06],\n",
              "             [-8.16675377e+00, -2.84194946e-04],\n",
              "             [-1.03420935e+01, -3.24249268e-05]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtGk-na1d0x6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "8db96f87-2390-4688-ee6b-df41df6059cc"
      },
      "source": [
        "# turn probabilites into category predictions\n",
        "tmp_is_positive = tmp_pred[:,1] > tmp_pred[:,0]\n",
        "for i, p in enumerate(tmp_is_positive):\n",
        "    print(f\"Neg log prob {tmp_pred[i,0]:.4f}\\tPos log prob {tmp_pred[i,1]:.4f}\\t is positive? {p}\\t actual {tmp_targets[i]}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neg log prob -0.0771\tPos log prob -2.6003\t is positive? False\t actual 0\n",
            "Neg log prob -0.0000\tPos log prob -12.0189\t is positive? False\t actual 0\n",
            "Neg log prob -12.1895\tPos log prob -0.0000\t is positive? True\t actual 1\n",
            "Neg log prob -12.7901\tPos log prob -0.0000\t is positive? True\t actual 1\n",
            "Neg log prob -0.0000\tPos log prob -12.8114\t is positive? False\t actual 0\n",
            "Neg log prob -12.2819\tPos log prob -0.0000\t is positive? True\t actual 1\n",
            "Neg log prob -2.7928\tPos log prob -0.0632\t is positive? True\t actual 1\n",
            "Neg log prob -0.0000\tPos log prob -11.7257\t is positive? False\t actual 0\n",
            "Neg log prob -7.8442\tPos log prob -0.0004\t is positive? True\t actual 1\n",
            "Neg log prob -0.0000\tPos log prob -11.4506\t is positive? False\t actual 0\n",
            "Neg log prob -0.0002\tPos log prob -8.3788\t is positive? False\t actual 0\n",
            "Neg log prob -0.0000\tPos log prob -13.1816\t is positive? False\t actual 0\n",
            "Neg log prob -2.5017\tPos log prob -0.0855\t is positive? True\t actual 0\n",
            "Neg log prob -12.4685\tPos log prob -0.0000\t is positive? True\t actual 1\n",
            "Neg log prob -8.1668\tPos log prob -0.0003\t is positive? True\t actual 1\n",
            "Neg log prob -10.3421\tPos log prob -0.0000\t is positive? True\t actual 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1AP0ifNeDz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy(preds, y, y_weights):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        preds: a tensor of shape (dim_batch, output_dim) \n",
        "        y: a tensor of shape (dim_batch, output_dim) with the true labels\n",
        "        y_weights: a n.ndarray with the a weight for each example\n",
        "    Output: \n",
        "        accuracy: a float between 0-1 \n",
        "        weighted_num_correct (np.float32): Sum of the weighted correct predictions\n",
        "        sum_weights (np.float32): Sum of the weights\n",
        "    \"\"\"\n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    # Create an array of booleans, \n",
        "    # True if the probability of positive sentiment is greater than\n",
        "    # the probability of negative sentiment\n",
        "    # else False\n",
        "    is_pos = preds[:,1] > preds[:,0]\n",
        "\n",
        "    # convert the array of booleans into an array of np.int32\n",
        "    is_pos_int = is_pos.astype(np.int32)\n",
        "    \n",
        "    # compare the array of predictions (as int32) with the target (labels) of type int32\n",
        "    correct = is_pos_int == np.array(y)\n",
        "\n",
        "    # Count the sum of the weights.\n",
        "    sum_weights = np.sum(y_weights)\n",
        "    \n",
        "    # convert the array of correct predictions (boolean) into an arrayof np.float32\n",
        "    correct_float = correct.astype(np.float32)\n",
        "    \n",
        "    # Multiply each prediction with its corresponding weight.\n",
        "    weighted_correct_float = correct_float * np.array(y_weights)\n",
        "\n",
        "    # Sum up the weighted correct predictions (of type np.float32), to go in the\n",
        "    # denominator.\n",
        "    weighted_num_correct = np.sum(weighted_correct_float)\n",
        " \n",
        "    # Divide the number of weighted correct predictions by the sum of the\n",
        "    # weights.\n",
        "    accuracy = weighted_num_correct / sum_weights\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return accuracy, weighted_num_correct, sum_weights"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oewTsfNaePA6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "85b741a8-33af-4592-b8b0-20717df6f1a3"
      },
      "source": [
        "# test your function\n",
        "tmp_val_generator = val_generator(64)\n",
        "\n",
        "# get one batch\n",
        "tmp_batch = next(tmp_val_generator)\n",
        "\n",
        "# Position 0 has the model inputs (tweets as tensors)\n",
        "# position 1 has the targets (the actual labels)\n",
        "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
        "\n",
        "# feed the tweet tensors into the model to get a prediction\n",
        "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
        "\n",
        "tmp_acc, tmp_num_correct, tmp_num_predictions = compute_accuracy(preds=tmp_pred, y=tmp_targets, y_weights=tmp_example_weights)\n",
        "\n",
        "print(f\"Model's prediction accuracy on a single training batch is: {100 * tmp_acc}%\")\n",
        "print(f\"Weighted number of correct predictions {tmp_num_correct}; weighted number of total observations predicted {tmp_num_predictions}\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model's prediction accuracy on a single training batch is: 95.3125%\n",
            "Weighted number of correct predictions 61.0; weighted number of total observations predicted 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UItRdig5eSUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(generator, model):\n",
        "    '''\n",
        "    Input: \n",
        "        generator: an iterator instance that provides batches of inputs and targets\n",
        "        model: a model instance \n",
        "    Output: \n",
        "        accuracy: float corresponding to the accuracy\n",
        "    '''\n",
        "    \n",
        "    accuracy = 0.\n",
        "    total_num_correct = 0\n",
        "    total_num_pred = 0\n",
        "    \n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    for batch in generator: \n",
        "        \n",
        "        # Retrieve the inputs from the batch\n",
        "        inputs = batch[0]\n",
        "        \n",
        "        # Retrieve the targets (actual labels) from the batch\n",
        "        targets = batch[1]\n",
        "        \n",
        "        # Retrieve the example weight.\n",
        "        example_weight = batch[2]\n",
        "\n",
        "        # Make predictions using the inputs\n",
        "        pred = model(inputs)\n",
        "        \n",
        "        # Calculate accuracy for the batch by comparing its predictions and targets\n",
        "        batch_accuracy, batch_num_correct, batch_num_pred = compute_accuracy(preds=pred, y=targets, y_weights=example_weight)\n",
        "        \n",
        "        # Update the total number of correct predictions\n",
        "        # by adding the number of correct predictions from this batch\n",
        "        total_num_correct += batch_num_correct\n",
        "        \n",
        "        # Update the total number of predictions \n",
        "        # by adding the number of predictions made for the batch\n",
        "        total_num_pred += batch_num_pred\n",
        "\n",
        "    # Calculate accuracy over all examples\n",
        "    accuracy = total_num_correct/total_num_pred\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    return accuracy"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjvTtsJFekJI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d965c0e-fde9-42ca-bbe6-1f470e9d1d46"
      },
      "source": [
        "model = training_loop.eval_model\n",
        "accuracy = test_model(test_generator(16), model)\n",
        "\n",
        "print(f'The accuracy of your model on the validation set is {accuracy:.4f}', )"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of your model on the validation set is 0.9427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JvGFkUqenqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(sentence):\n",
        "    inputs = np.array(text_to_tensor(sentence, vocab_dict=Vocab))\n",
        "    \n",
        "    # Batch size 1, add dimension for batch, to work with the model\n",
        "    inputs = inputs[None, :]  \n",
        "    \n",
        "    # predict with the model\n",
        "    preds_probs = model(inputs)\n",
        "    \n",
        "    # Turn probabilities into categories\n",
        "    preds = int(preds_probs[0, 1] > preds_probs[0, 0])\n",
        "    \n",
        "    sen_pred = \"Not Fake\"\n",
        "    if preds == 1:\n",
        "        sen_pred = 'Fake'\n",
        "        \n",
        "    print(\"Fake \",str(preds_probs[0, 1]), \" Not Fake \",str(preds_probs[0, 0]))\n",
        "\n",
        "    return preds, sen_pred"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZG_pyBefSeM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "47570abd-cb49-4b7e-b766-79a8847e4fc6"
      },
      "source": [
        "sentence = \"Donald John Trump (born June 14, 1946) is the 45th and current president of the United States. Before entering politics, he was a businessman and television personality. Trump was born and raised in Queens, a borough of New York City. He attended Fordham University for two years and received a bachelor's degree in economics from the Wharton School of the University of Pennsylvania. He became president of his father's real-estate business in 1971, renamed it The Trump Organization, and expanded its operations to building or renovating skyscrapers, hotels, casinos, and golf courses. Trump later started various side ventures, mostly by licensing his name. Trump and his businesses have been involved in more than 4,000 state and federal legal actions, including six bankruptcies. He owned the Miss Universe brand of beauty pageants from 1996 to 2015. He produced and hosted The Apprentice, a reality television series, from 2003 to 2015. As of April 2020, Forbes estimated his net worth to be $2.1 billion.\"\n",
        "tmp_pred, tmp_sen_pred = predict(sentence)\n",
        "print(f\"The sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sen_pred}.\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fake  -7.9701853  Not Fake  -0.00034570694\n",
            "The sentence \n",
            "***\n",
            "\"Donald John Trump (born June 14, 1946) is the 45th and current president of the United States. Before entering politics, he was a businessman and television personality. Trump was born and raised in Queens, a borough of New York City. He attended Fordham University for two years and received a bachelor's degree in economics from the Wharton School of the University of Pennsylvania. He became president of his father's real-estate business in 1971, renamed it The Trump Organization, and expanded its operations to building or renovating skyscrapers, hotels, casinos, and golf courses. Trump later started various side ventures, mostly by licensing his name. Trump and his businesses have been involved in more than 4,000 state and federal legal actions, including six bankruptcies. He owned the Miss Universe brand of beauty pageants from 1996 to 2015. He produced and hosted The Apprentice, a reality television series, from 2003 to 2015. As of April 2020, Forbes estimated his net worth to be $2.1 billion.\"\n",
            "***\n",
            "is Not Fake.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2UFDL1zffWA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "3ba0a555-b919-404b-93e1-c2e9d7ea063c"
      },
      "source": [
        "sentence = all_neg_text[-1]\n",
        "tmp_pred, tmp_sen_pred = predict(sentence)\n",
        "print(f\"The sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sen_pred}.\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fake  -12.409691  Not Fake  -3.8146973e-06\n",
            "The sentence \n",
            "***\n",
            "\"The Macy’s of today grew from the union of several great names in American retailing, including its namesake chain, Bloomingdale’s and Marshall Field’s. But the ambitious owner of Saks Fifth Avenue has broached the idea of taking the union even further, combining with Macy’s to create a department store juggernaut at a time when the industry is reeling. Hudson’s Bay Company, the Canadian owner of Saks, has approached Macy’s about a potential takeover, people briefed on the matter who were not authorized to speak publicly said on Friday. Talks between the two companies are at an early stage and may still fall apart or lead to a partnership of some kind rather than a sale. While it is unclear whether a deal will happen, a combination could lift the fortunes of Macy’s, the country’s biggest department store, which has been struggling. Investors certainly appeared to see it that way. Shares of Macy’s rose as much as 12 percent on Friday, its biggest intraday gain since Aug. 11, according to data from Bloomberg. Once a retail titan, Macy’s has struggled to remain relevant as   and discount retailers have decimated the traditional    business. Last month, Macy’s announced plans to cut more than 10, 000 jobs and close some of its 880 stores. Terry Lundgren, its chief executive and the architect of Macy’s last big merger, is expected to step down by the end of March. He will be succeeded by the company’s president, Jeffrey Gennette. Since the recession, shoppers have grown accustomed to hunt for bargains and to not pay full price. Discount stores and outlet malls have flourished. Traditional stores have been compelled to respond by trimming prices, which cuts into their margins. Departments stores have been hit especially hard, particularly as shoppers migrate away from malls. What has emerged, analysts say, is a virtual race to the bottom. That has been particularly difficult for Macy’s, born of a series of mergers over the past two decades that made it a juggernaut in the industry. A stalwart of the middle tier of retail, the company has neither the advantages of   retailers like HM nor the   stores. In addition, Macy’s faces increasingly fierce competition online from sites like Amazon and elsewhere. Macy’s troubles have drawn the attention of a prominent activist hedge fund, Starboard Value, which has urged the company to generate cash by selling the real estate beneath its stores. Starboard, which held just under 1 percent of Macy’s shares as of Sept. 30, had previously estimated the value of that land at about $21 billion. On Friday, analysts at Citigroup estimated that Macy’s   holdings could be worth at least $18 billion. Macy’s market value, by comparison, was just under $11 billion as of Friday morning. Macy’s has taken some steps to sell or redevelop stores, and last year, it added an expert on real estate transactions to its board. But the company has largely resisted more ambitious efforts to divest its real estate, including     deals, in which a company sells the underlying land beneath its stores and then rents it back. The company’s suitor, Hudson’s Bay Company, is far smaller  —   its market value was about 1. 9 billion Canadian dollars, or $1. 5 billion  —   but is known for its bold steps. Hudson’s Bay Company has assembled a growing empire that includes the Hudson’s Bay department store chain, Lord  Taylor and its crown jewel, Saks. And the governor and executive chairman of the Hudson’s Bay Company, Richard Baker, has shown little fear of using debt: In November 2014, the company borrowed nearly $4 billion against the Saks flagship in Midtown Manhattan. He has spoken often of retailers’ need to highlight the value of their real estate. Financing a bid for Macy’s may be trickier, however, because the it carries about $6. 5 billion in   debt. That may mean that the Hudson’s Bay Company will have to bring in a partner or borrow against more of its real estate holdings. A spokesman for the Hudson’s Bay Company declined to comment on the talks, which were reported earlier by The Wall Street Journal. “We do not comment on rumors and speculation,” a representative for Macy’s said. A representative for Starboard Value did not respond to a request for comment. Some analysts said that they saw the merit of a potential combination, particularly given Macy’s operational woes and Mr. Baker’s expertise in wringing money out of real estate. “There is a clear logic, despite disparity in   cap” between Macy’s and Hudson’s Bay Company, Craig Johnson, the president of Customer Growth Partners, a research firm, said in a note. Referring to Macy’s stock ticker symbol, he added, “The retail market has been changing faster than M has been able to keep up with, whether the flight from the mall or the migration online. ”\"\n",
            "***\n",
            "is Not Fake.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHyKTZd0RmsP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "91c3f623-0a1b-4833-9883-7ce282cabc81"
      },
      "source": [
        "sentence = all_pos_text[-1]\n",
        "tmp_pred, tmp_sen_pred = predict(sentence)\n",
        "print(f\"The sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sen_pred}.\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fake  -1.7642975e-05  Not Fake  -10.937075\n",
            "The sentence \n",
            "***\n",
            "\"  David Swanson is an author, activist, journalist, and radio host. He is a 2015 Nobel Peace Prize Nominee. He is director of WorldBeyondWar.com and campaign coordinator for RootsAction.org . He hosts Talk Nation Radio . Talk Nation Radio is on VT Radio and is syndicated by Pacifica Network. The show also airs on WTJU, Charlottesville, VA; WCSX-Detroit, MI; KGHI, Westport, WA; WHUS, Storrs, CT; WPRR, Grand Rapids, MI; KRFP-LP, Moscow, ID; KZGM, Cabool, MO; KMUD, Garberville, CA; WAZU, Peoria, IL; WXRD, Crown Point, IN; Geneva Radio, Geneva, NY; KKRN, Round Mountain, CA; KSKQ-LP, Ashland, OR; WUOW-LP, Oneonta, NY; No Lies Radio, Pinole, CA; WYAP-LP, Clay, WV; The Detour, Johnson City, TN; WZRD, Chicago, IL; WEFT, Champaign, IL; WXPI, Pittsburgh, PA; WDRT, Viroqua, WI; Veracity Now, online; Liberty and Justice Radio, Shirley, MA; Ithaca Community Radio, Ithaca, NY; WMCB, Greenfield, MA; PRX.org; KAOS 89.3fm, Olympia, WA; WUSB 90.1 FM, Stony Brook, NY; WOOL-FM, Bellow Falls, Vermont; WSLR-LP 96.5 in Sarasota, Florida. He also blogs at DavidSwanson.org and WarIsACrime.org And is a prolific author. His latest books are; War Is A Lie , Daybreak: Undoing the Imperial Presidency and Forming a More Perfect Union , and When the World Outlawed War Swanson holds a master's degree in philosophy from the University of Virginia. He has worked as a newspaper reporter and as a communications director, with jobs including press secretary for Dennis Kucinich's 2004 presidential campaign, media coordinator for the International Labor Communications Association, and three years as communications coordinator for ACORN, the Association of Community Organizations for Reform Now. Read his full and complete biography at DavidSwanson.org and also visit book site at War Is Crime . What Keeps the F-35 Alive By David Swanson on October 31, 2016 Petition to Stop F-35 Going Global \n",
            "by David Swanson \n",
            "Imagine if a local business in your town invented a brand new tool that was intended to have an almost magical effect thousands of miles away. However, where the tool was kept and used locally became an area unsafe for children. Children who got near this tool tended to have increased blood pressure and increased stress hormones, lower reading skills, poorer memories, impaired auditory and speech perception, and impaired academic performance. \n",
            "Most of us would find this situation at least a little concerning, unless the new invention was designed to murder lots of people. Then it’d be just fine. \n",
            "Now, imagine if this same new tool ruined neighborhoods because people couldn’t safely live near it. Imagine if the government had to compensate people but kick them out of living near the location of this tool. Again, I think, we might find that troubling if mass murder were not the mission. \n",
            "Imagine also that this tool fairly frequently explodes, emitting highly toxic chemicals, particles, and fibers unsafe to breathe into the air for miles around. Normally, that’d be a problem. But if this tool is needed for killing lots of people, we’ll work with its flaws, won’t we? \n",
            "Now, what if this new gadget was expected to cost at least $1,400,000,000,000 over 50 years? And what if that money had to be taken away from numerous other expenses more beneficial for the economy and the world? What if the $1.4 trillion was drained out of the economy causing a loss of jobs and a radical diminuition of resources for education, healthcare, housing, environmental protection, or humanitarian aid? Wouldn’t that be a worry in some cases, I mean in those cases where the ability to kill tons of human beings wasn’t at stake? \n",
            "What if this product, even when working perfectly, was a leading destroyer of the earth’s natural environment? \n",
            "What if this high-tech toy wasn’t even designed to do what was expected of it and wasn’t even able to do what it was designed for? \n",
            "Amazingly, even those shortcomings do not matter as long as the intention is massive murder and destruction. Then, all is forgiven. \n",
            "The tool I’m describing is called the F-35. At RootsAction.org you can find a new petition launched by locally-minded people acting globally in places where the F-35 is intended to be based. Also at that link you’ll find explanations of how the tool I’ve been decribing is the F-35. \n",
            "The petition is directed to the United States Congress and the governments of Australia, Italy, the Netherlands, Norway, Turkey, the United Kingdom, Israel, Japan and South Korea from the world and from the people of Burlington, Vermont, and Fairbanks, Alaska, where the F-35 is to be based. This effort is being initiated by Vermont Stop the F35 Coalition, Save Our Skies Vermont, Western Maine Matters, Alaska Peace Center, University of Alaska Fairbanks Peace Club, North Star Chapter 146 Veterans For Peace, World Beyond War, RootsAction.org, Code Pink, and Ben Cohen. \n",
            "The petition reads: \n",
            "The F-35 is a weapon of offensive war, serving no defensive purpose. It is planned to cost the U.S. $1.4 trillion over 50 years. Because starvation on earth could be ended for $30 billion and the lack of clean drinking water for $11 billion per year, it is first and foremost through the wasting of resources that this airplane will kill. Military spending, contrary to popular misconception, also hurts the U.S. economy ( see here ) and other economies. The F-35 causes negative health impacts and cognitive impairment in children living near its bases. It renders housing near airports unsuitable for residential use. It has a high crash rate and horrible consequences to those living in the area of its crashes. Its emissions are a major environmental polluter. \n",
            "Wars are endangering the United States and other participating nations rather than protecting them. Nonviolent tools of law, diplomacy, aid, crisis prevention, and verifiable nuclear disarmament should be substituted for continuing counterproductive wars. Therefore, we, the undersigned, call for the immediate cancellation of the F-35 program as a whole, and the immediate cancellation of plans to base any such dangerous and noisy jets near populated areas. We oppose replacing the F-35 with any other weapon or basing the F-35 in any other locations. We further demand redirection of the money for the F-35 back into taxpayers’ pockets, and into environmental and human needs in the U.S., other F-35 customer nations, and around the world, including to fight climate change, pay off student debt, rebuild crumbling infrastructure, and improve education, healthcare, and housing. \n",
            "Add your name . \n",
            "David Swanson is an author, activist, journalist, and radio host. He is director of WorldBeyondWar.org and campaign coordinator for RootsAction.org . Swanson’s books include War Is A Lie . He blogs at DavidSwanson.org and WarIsACrime.org . He hosts Talk Nation Radio .He is a 2015 and 2016 Nobel Peace Prize Nominee. \n",
            "Follow him on Twitter: @davidcnswanson and FaceBook . \n",
            "Help support DavidSwanson.org, WarIsACrime.org, and TalkNationRadio.org by clicking here: http://davidswanson.org/donate .\"\n",
            "***\n",
            "is Fake.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xAyXDZuRqhn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "f4e7d15e-11e8-404d-d092-2c76ace6c0b7"
      },
      "source": [
        "sentence = \"The first known use of the name \\\"America\\\" dates back to 1507, when it appeared on a world map created by the German cartographer Martin Waldseemüller. It comes from the surname of Italian explorer Amerigo Vespucci, who was the first to postulate that the West Indies did not represent Asia's eastern limit, but part of a previously unknown landmass.[20][21] The full name \\\"United States of America\\\" dates to the Revolutionary War. The second draft of the Articles of Confederation, prepared by John Dickinson, and Thomas Jefferson's original rough draft of the Declaration of Independence both included the name. However, it is unclear which of these two usages came first\"\n",
        "tmp_pred, tmp_sen_pred = predict(sentence)\n",
        "print(f\"The sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sen_pred}.\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fake  -4.7683716e-06  Not Fake  -12.244802\n",
            "The sentence \n",
            "***\n",
            "\"The first known use of the name \"America\" dates back to 1507, when it appeared on a world map created by the German cartographer Martin Waldseemüller. It comes from the surname of Italian explorer Amerigo Vespucci, who was the first to postulate that the West Indies did not represent Asia's eastern limit, but part of a previously unknown landmass.[20][21] The full name \"United States of America\" dates to the Revolutionary War. The second draft of the Articles of Confederation, prepared by John Dickinson, and Thomas Jefferson's original rough draft of the Declaration of Independence both included the name. However, it is unclear which of these two usages came first\"\n",
            "***\n",
            "is Fake.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OW6l10uSeEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 49,
      "outputs": []
    }
  ]
}